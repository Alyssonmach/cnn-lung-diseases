{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "colab": {
      "name": "experiment5_dataset4.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weekly-aerospace"
      },
      "source": [
        "# Experimento 5 \n",
        "***\n",
        "- Rede utilizada: Inception\n",
        "- Conjunto de Dados: VinBigData\n",
        "- Modelando uma para classificar problemas pulmonares em 11 classes distintas"
      ],
      "id": "weekly-aerospace"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "current-following"
      },
      "source": [
        "### Importação dos pacotes"
      ],
      "id": "current-following"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meaningful-iraqi"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "#from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "meaningful-iraqi",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "banned-comfort"
      },
      "source": [
        "### Pré-processamento nos dados"
      ],
      "id": "banned-comfort"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heated-adult"
      },
      "source": [
        "# lendo os dados de um arquivo csv\n",
        "dataframe = pd.read_csv('/content/drive/MyDrive/vinbigdata/train.csv')\n",
        "# criando uma coluna com os caminhos relativos as imagens\n",
        "dataframe['image_path'] = '/content/drive/MyDrive/vinbigdata/train/' + dataframe.image_id + '.jpg'"
      ],
      "id": "heated-adult",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "starting-setting",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a83f789-04d2-4a75-ac17-57bcbc6ec7f1"
      },
      "source": [
        "print('total de imagens disponíveis:', str(len(set(dataframe['image_path']))))"
      ],
      "id": "starting-setting",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total de imagens disponíveis: 15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tough-newfoundland",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4106614c-a128-4817-989e-8589c647cca8"
      },
      "source": [
        "# visualizando os casos disponíveis\n",
        "dataframe['class_name'].value_counts()"
      ],
      "id": "tough-newfoundland",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "No finding            31818\n",
              "Aortic enlargement     7162\n",
              "Cardiomegaly           5427\n",
              "Pleural thickening     4842\n",
              "Pulmonary fibrosis     4655\n",
              "Nodule/Mass            2580\n",
              "Lung Opacity           2483\n",
              "Pleural effusion       2476\n",
              "Other lesion           2203\n",
              "Infiltration           1247\n",
              "ILD                    1000\n",
              "Calcification           960\n",
              "Consolidation           556\n",
              "Atelectasis             279\n",
              "Pneumothorax            226\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "intermediate-anchor"
      },
      "source": [
        "# removendo os casos não relativos a distúrbios pulmonares\n",
        "dataframe = dataframe[dataframe.class_name != 'Aortic enlargement']\n",
        "dataframe = dataframe[dataframe.class_name != 'Cardiomegaly']\n",
        "dataframe = dataframe[dataframe.class_name != 'Other lesion']"
      ],
      "id": "intermediate-anchor",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "retained-message",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556f51d8-b771-4cb6-ed17-83cc7f782a8d"
      },
      "source": [
        "# separando os casos rotulados como normais e anormais\n",
        "normal_cases = dataframe[(dataframe.class_id == 14) & (dataframe.class_name == 'No finding')]\n",
        "abnormal_cases = dataframe[(dataframe.class_id != 14) & (dataframe.class_name != 'No finding')]\n",
        "\n",
        "print('total de dados após a filtração:', str(len(set(normal_cases['image_path'])) + len(set(abnormal_cases['image_path']))))"
      ],
      "id": "retained-message",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total de dados após a filtração: 13952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forty-system"
      },
      "source": [
        "# removendo as imagens repetidas\n",
        "normal_data = normal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
        "abnormal_data = abnormal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
        "\n",
        "# criando dataframes especifos com caminhos para as imagens e rótulos\n",
        "normal_data['target'] = 'normal'\n",
        "abnormal_data['target'] = 'abnormal'"
      ],
      "id": "forty-system",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viral-conspiracy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17192d6-249b-46e2-cca3-810ac6433742"
      },
      "source": [
        "print('quantidade de dados rotulados como normais:', len(normal_data))\n",
        "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
      ],
      "id": "viral-conspiracy",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de dados rotulados como normais: 10606\n",
            "quantidade de dados rotulados como anormais: 3346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baking-shield"
      },
      "source": [
        "# removendo 69% dos casos normais para balancear os dados\n",
        "normal, _ = train_test_split(normal_data, test_size = 0.69, random_state = 42)"
      ],
      "id": "baking-shield",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "little-communications",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e1bfb6-0caa-4119-d3be-480611614483"
      },
      "source": [
        "print('quantidade de dados rotulados como normais:', len(normal))\n",
        "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
      ],
      "id": "little-communications",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de dados rotulados como normais: 3287\n",
            "quantidade de dados rotulados como anormais: 3346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flying-supervision"
      },
      "source": [
        "# concatenando os dataframes de casos normais e anormais\n",
        "full_data = abnormal_data"
      ],
      "id": "flying-supervision",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRS6Rb4Oj-t6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1636d03-2d96-4d83-cc64-7b2405fe6356"
      },
      "source": [
        "# visualizando a quantidade de exemplos por classe disponíveis\n",
        "full_data['class_name'].value_counts()"
      ],
      "id": "QRS6Rb4Oj-t6",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pleural thickening    901\n",
              "Pulmonary fibrosis    742\n",
              "Lung Opacity          427\n",
              "Nodule/Mass           339\n",
              "Pleural effusion      328\n",
              "Calcification         167\n",
              "Infiltration          163\n",
              "ILD                   152\n",
              "Consolidation          59\n",
              "Atelectasis            41\n",
              "Pneumothorax           27\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKZYDuypT5-W"
      },
      "source": [
        "# importando os dataframes dos dados de treinamento, validação e teste\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/train_df.csv', sep = ',', index_col=  0)\n",
        "validation_df = pd.read_csv('/content/drive/MyDrive/validation_df.csv', sep = ',', index_col=  0)\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/test_df.csv' , sep = ',', index_col=  0)"
      ],
      "id": "GKZYDuypT5-W",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n3QarWhUUr9"
      },
      "source": [
        "# tornando as classes na coluna 'labels' categórica\n",
        "train_df.loc[train_df.labels == 1, 'labels'] = 'abnormal'\n",
        "train_df.loc[train_df.labels == 0, 'labels'] = 'normal'\n",
        "\n",
        "validation_df.loc[validation_df.labels == 1, 'labels'] = 'abnormal'\n",
        "validation_df.loc[validation_df.labels == 0, 'labels'] = 'normal'\n",
        "\n",
        "test_df.loc[test_df.labels == 1, 'labels'] = 'abnormal'\n",
        "test_df.loc[test_df.labels == 0, 'labels'] = 'normal'"
      ],
      "id": "4n3QarWhUUr9",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxIV_-VxUXdo"
      },
      "source": [
        "# separando os casos de anormalidade no dataset nih\n",
        "train_abnormal_cases = train_df[train_df.labels == 'abnormal']\n",
        "validation_abnormal_cases = validation_df[validation_df.labels == 'abnormal']\n",
        "test_abnormal_cases = test_df[test_df.labels == 'abnormal']"
      ],
      "id": "BxIV_-VxUXdo",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pSQO3nvUk_O"
      },
      "source": [
        "# concatenando os dados do dataset nih\n",
        "nih_data = pd.concat([train_abnormal_cases, validation_abnormal_cases,\n",
        "                      test_abnormal_cases])"
      ],
      "id": "1pSQO3nvUk_O",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUVi7bt1V6wO"
      },
      "source": [
        "# organizando o nome das colunas\n",
        "nih_data.rename(columns= {'Image Index': 'image_path', 'finding_labels': 'class_name',\n",
        "                          'labels': 'target'}, inplace = True)"
      ],
      "id": "rUVi7bt1V6wO",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svw6fI2JW0ye"
      },
      "source": [
        "# concatenando os dados do vinbigdata e do nih\n",
        "full_data = pd.concat([full_data, nih_data])"
      ],
      "id": "svw6fI2JW0ye",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCLoX1KPXXm1"
      },
      "source": [
        "# removendo classes ineficientes para o aprendizado da rede \n",
        "full_data = full_data[(full_data.class_name != 'ILD') & (full_data.class_name != 'Calcification')]\n",
        "full_data = full_data[(full_data.class_name != 'Pneumonia') & (full_data.class_name != 'Pleural effusion')]\n",
        "full_data = full_data[(full_data.class_name != 'Nodule/Mass') & (full_data.class_name != 'Lung Opacity')]\n",
        "full_data = full_data[full_data.class_name != 'Pleural_Thickening']"
      ],
      "id": "cCLoX1KPXXm1",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnld5AI6Ya4d"
      },
      "source": [
        "# separando os dados de cada uma das classes\n",
        "infiltration = full_data[full_data.class_name == 'Infiltration']\n",
        "atelectasis = full_data[full_data.class_name == 'Atelectasis']\n",
        "nodule = full_data[full_data.class_name == 'Nodule']\n",
        "pneumothorax = full_data[full_data.class_name == 'Pneumothorax']\n",
        "consolidation = full_data[full_data.class_name == 'Consolidation']\n",
        "rest_abnormal_data = full_data[(full_data.class_name == 'Pleural thickening') |\n",
        "                               (full_data.class_name == 'Emphysema') |\n",
        "                               (full_data.class_name == 'Pulmonary fibrosis') |\n",
        "                               (full_data.class_name == 'Fibrosis') |\n",
        "                               (full_data.class_name == 'Edema')]"
      ],
      "id": "fnld5AI6Ya4d",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xYjJi_AanzW"
      },
      "source": [
        "# organizando a quantidade de dados disponíveis em cada uma das classes\n",
        "infiltration, _ = train_test_split(infiltration, test_size = 0.9, random_state = 42)\n",
        "atelectasis, _ = train_test_split(atelectasis, test_size = 0.77, random_state = 42)\n",
        "nodule, _ = train_test_split(nodule, test_size = 0.64, random_state = 42)\n",
        "pneumothorax, _ = train_test_split(pneumothorax, test_size = 0.56, random_state = 42)\n",
        "consolidation, _ = train_test_split(consolidation, test_size = 0.28, random_state = 42)\n",
        "full_data = pd.concat([infiltration, atelectasis, nodule, pneumothorax, consolidation, rest_abnormal_data])"
      ],
      "id": "_xYjJi_AanzW",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejs7xDFAcNGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d70c9bb-8888-48f9-b1e0-09fa6871c9a3"
      },
      "source": [
        "# visualizando o resultado final da quantidade de exemplos disponíveis em cada classe\n",
        "full_data['class_name'].value_counts()"
      ],
      "id": "Ejs7xDFAcNGs",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Consolidation         985\n",
              "Atelectasis           978\n",
              "Pneumothorax          977\n",
              "Nodule                973\n",
              "Infiltration          971\n",
              "Pleural thickening    901\n",
              "Emphysema             892\n",
              "Pulmonary fibrosis    742\n",
              "Fibrosis              727\n",
              "Edema                 628\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worst-management"
      },
      "source": [
        "# separando os dados de treinamento e de teste\n",
        "train_df, test_df = train_test_split(full_data, stratify = full_data['target'],\n",
        "                                     test_size = 0.15, random_state = 42)"
      ],
      "id": "worst-management",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "future-weather"
      },
      "source": [
        "# separando os dados de validação dos dados de treinamento\n",
        "train_df, validation_df = train_test_split(train_df, stratify = train_df['target'],\n",
        "                                           test_size = 0.25, random_state = 42)"
      ],
      "id": "future-weather",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secret-session",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61777554-a37f-4ed1-f540-556e3d0bebb7"
      },
      "source": [
        "# visualizando a quantidade de dados\n",
        "print('quantidade de imagens de treinamento:', len(train_df['image_path']))\n",
        "print('quantidade de rótulos de treinamento:', len(train_df['target']))\n",
        "print('quantidade de imagens de teste:', len(test_df['image_path']))\n",
        "print('quantidade de rótulos de teste:', len(test_df['target']))\n",
        "print('quantidade de imagens de validação:', len(validation_df['image_path']))\n",
        "print('quantidade de rótulos de validação:', len(validation_df['target']))"
      ],
      "id": "secret-session",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de imagens de treinamento: 5592\n",
            "quantidade de rótulos de treinamento: 5592\n",
            "quantidade de imagens de teste: 1317\n",
            "quantidade de rótulos de teste: 1317\n",
            "quantidade de imagens de validação: 1865\n",
            "quantidade de rótulos de validação: 1865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f8i2GizdDEU"
      },
      "source": [
        "# organizando um dicionário para realizar o balanceamento nos dados das classes\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(train_df['class_name']),\n",
        "                                                  train_df['class_name'])\n",
        "class_weight = {0: class_weights[0], 1: class_weights[1], 2: class_weights[2], 3: class_weights[3],\n",
        "                4: class_weights[4], 5: class_weights[5], 6: class_weights[6], 7: class_weights[7],\n",
        "                8: class_weights[8], 9: class_weights[9]}"
      ],
      "id": "1f8i2GizdDEU",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bulgarian-staff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f277b48-e108-4575-f322-60f353d64675"
      },
      "source": [
        "# normalizando as imagens de treinamento e aplicando aumento de dados\n",
        "image_generator = ImageDataGenerator(rescale = 1./255.,\n",
        "                                     rotation_range = 10, zoom_range = 0.2)\n",
        "\n",
        "# criando o gerador de imagens de treinamento \n",
        "train_generator = image_generator.flow_from_dataframe(\n",
        "                                                      dataframe = train_df,\n",
        "                                                      directory = '',\n",
        "                                                      x_col = 'image_path',\n",
        "                                                      y_col = 'class_name',\n",
        "                                                      batch_size = 32,\n",
        "                                                      seed = 42,\n",
        "                                                      shuffle = True,\n",
        "                                                      class_mode = 'categorical',\n",
        "                                                      target_size = (224, 224))\n",
        "\n",
        "# normalizando as imagens de teste e validação\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255.)\n",
        "\n",
        "# criando o gerador de imagens de validação \n",
        "valid_generator = test_datagen.flow_from_dataframe(\n",
        "                                                      dataframe = validation_df,\n",
        "                                                      directory = '.', \n",
        "                                                      x_col = 'image_path',\n",
        "                                                      y_col = 'class_name',\n",
        "                                                      batch_size = 32,\n",
        "                                                      seed = 42,\n",
        "                                                      shuffle = True,\n",
        "                                                      class_mode = 'categorical',\n",
        "                                                      target_size = (224, 224))\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "                                                  dataframe = test_df, \n",
        "                                                  directory = '.',\n",
        "                                                  x_col = 'image_path',\n",
        "                                                  y_col = 'class_name',\n",
        "                                                  batch_size = 32,\n",
        "                                                  seed = 42,\n",
        "                                                  shuffle = True,\n",
        "                                                  class_mode = 'categorical',\n",
        "                                                  target_size = (224, 224))"
      ],
      "id": "bulgarian-staff",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5588 validated image filenames belonging to 10 classes.\n",
            "Found 1865 validated image filenames belonging to 10 classes.\n",
            "Found 1317 validated image filenames belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rolled-luxury"
      },
      "source": [
        "### Preparando a rede neural convolucional"
      ],
      "id": "rolled-luxury"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEjKBnM-DfVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ddd534-f5bf-4eea-d8f2-63f7703f59ec"
      },
      "source": [
        "# realizando transferência de aprendizado com a mobile net\n",
        "model = MobileNetV2(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')"
      ],
      "id": "vEjKBnM-DfVb",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1V-vMWlE8UM"
      },
      "source": [
        "# obtendo a última camada como sendo a nomeada por 'mixed7'\n",
        "last_layer = model.get_layer('out_relu')\n",
        "last_output = last_layer.output\n",
        "# transformandos os mapas de característica em um vetor 1-dimensional\n",
        "x = layers.GlobalAveragePooling2D()(last_output)\n",
        "# adicionando uma camada de saída com um neurônio e uma função de ativação sigmoide\n",
        "x = layers.Dense(units = 10, activation = tf.nn.softmax)(x)      \n",
        "# conecatando as camadas definidas acima com a arquitetura inception\n",
        "model = Model(model.input, x) \n",
        "# compilando a rede \n",
        "model.compile(optimizer = optimizers.RMSprop(learning_rate = 0.0001), loss = 'categorical_crossentropy', \n",
        "              metrics = ['acc'])"
      ],
      "id": "g1V-vMWlE8UM",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "engaged-culture"
      },
      "source": [
        "# definindo o caminho pelo qual os pesos serão armazenados \n",
        "filepath = \"transferlearning_weights.hdf5\"\n",
        "# callback para salvar o melhor valor dos pesos em relação ao desempenho com os dados de validação \n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max')\n",
        "# definindo um array de callbacks\n",
        "callbacks = [checkpoint]"
      ],
      "id": "engaged-culture",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mighty-manufacturer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fc5c34-cf12-476d-830a-a9936afadba5"
      },
      "source": [
        "# treinando a rede neural convolucional\n",
        "history = model.fit_generator(train_generator, steps_per_epoch = 5587 // 32, \n",
        "                              validation_data = valid_generator, validation_steps = 1865 // 32,\n",
        "                              callbacks = callbacks, epochs = 150, class_weight = class_weight,\n",
        "                              use_multiprocessing = True, workers = 4)"
      ],
      "id": "mighty-manufacturer",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 2.0181 - acc: 0.2375WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 751s 4s/step - loss: 2.0172 - acc: 0.2377 - val_loss: 2.8080 - val_acc: 0.1557\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.15571, saving model to transferlearning_weights.hdf5\n",
            "Epoch 2/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 1.5804 - acc: 0.3853WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 576ms/step - loss: 1.5804 - acc: 0.3852 - val_loss: 2.3083 - val_acc: 0.1967\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.15571 to 0.19666, saving model to transferlearning_weights.hdf5\n",
            "Epoch 3/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 1.4814 - acc: 0.4209WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 101s 570ms/step - loss: 1.4813 - acc: 0.4209 - val_loss: 2.4035 - val_acc: 0.2058\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.19666 to 0.20582, saving model to transferlearning_weights.hdf5\n",
            "Epoch 4/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 1.3552 - acc: 0.4788WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 577ms/step - loss: 1.3553 - acc: 0.4787 - val_loss: 2.2519 - val_acc: 0.2322\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.20582 to 0.23222, saving model to transferlearning_weights.hdf5\n",
            "Epoch 5/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 1.2443 - acc: 0.5231WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 579ms/step - loss: 1.2444 - acc: 0.5231 - val_loss: 2.0995 - val_acc: 0.2592\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.23222 to 0.25916, saving model to transferlearning_weights.hdf5\n",
            "Epoch 6/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 1.1214 - acc: 0.5795WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 575ms/step - loss: 1.1217 - acc: 0.5794 - val_loss: 2.0688 - val_acc: 0.2586\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.25916\n",
            "Epoch 7/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 1.0586 - acc: 0.6020WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 578ms/step - loss: 1.0587 - acc: 0.6019 - val_loss: 2.3643 - val_acc: 0.2139\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.25916\n",
            "Epoch 8/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.9725 - acc: 0.6422WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 579ms/step - loss: 0.9727 - acc: 0.6421 - val_loss: 2.5121 - val_acc: 0.2462\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.25916\n",
            "Epoch 9/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.8713 - acc: 0.6710WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 583ms/step - loss: 0.8715 - acc: 0.6709 - val_loss: 2.5286 - val_acc: 0.2662\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.25916 to 0.26616, saving model to transferlearning_weights.hdf5\n",
            "Epoch 10/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.7913 - acc: 0.7097WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 576ms/step - loss: 0.7915 - acc: 0.7096 - val_loss: 2.4836 - val_acc: 0.2780\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.26616 to 0.27802, saving model to transferlearning_weights.hdf5\n",
            "Epoch 11/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.7206 - acc: 0.7375WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 101s 571ms/step - loss: 0.7206 - acc: 0.7374 - val_loss: 2.7941 - val_acc: 0.2602\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.27802\n",
            "Epoch 12/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.6461 - acc: 0.7637WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 101s 572ms/step - loss: 0.6461 - acc: 0.7637 - val_loss: 3.0994 - val_acc: 0.2360\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.27802\n",
            "Epoch 13/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.5548 - acc: 0.8042WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 577ms/step - loss: 0.5549 - acc: 0.8041 - val_loss: 3.3405 - val_acc: 0.2371\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.27802\n",
            "Epoch 14/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.5228 - acc: 0.8133WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 576ms/step - loss: 0.5228 - acc: 0.8133 - val_loss: 3.2315 - val_acc: 0.2398\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.27802\n",
            "Epoch 15/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.4496 - acc: 0.8409WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 578ms/step - loss: 0.4498 - acc: 0.8408 - val_loss: 3.3143 - val_acc: 0.2495\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.27802\n",
            "Epoch 16/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.4352 - acc: 0.8434WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 101s 572ms/step - loss: 0.4353 - acc: 0.8434 - val_loss: 4.1026 - val_acc: 0.2430\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.27802\n",
            "Epoch 17/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.3530 - acc: 0.8786WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 576ms/step - loss: 0.3531 - acc: 0.8785 - val_loss: 3.3505 - val_acc: 0.2597\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.27802\n",
            "Epoch 18/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.3445 - acc: 0.8790WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 579ms/step - loss: 0.3446 - acc: 0.8790 - val_loss: 3.4711 - val_acc: 0.2742\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.27802\n",
            "Epoch 19/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.3036 - acc: 0.8898WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 577ms/step - loss: 0.3037 - acc: 0.8898 - val_loss: 3.7871 - val_acc: 0.2468\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.27802\n",
            "Epoch 20/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.2956 - acc: 0.9043WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 579ms/step - loss: 0.2957 - acc: 0.9042 - val_loss: 4.0645 - val_acc: 0.2441\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.27802\n",
            "Epoch 21/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.2547 - acc: 0.9105WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 577ms/step - loss: 0.2548 - acc: 0.9104 - val_loss: 3.8191 - val_acc: 0.3033\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.27802 to 0.30334, saving model to transferlearning_weights.hdf5\n",
            "Epoch 22/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.2579 - acc: 0.9122WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 580ms/step - loss: 0.2579 - acc: 0.9122 - val_loss: 4.4362 - val_acc: 0.2683\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.30334\n",
            "Epoch 23/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.2303 - acc: 0.9236WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 581ms/step - loss: 0.2303 - acc: 0.9235 - val_loss: 3.8127 - val_acc: 0.2856\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.30334\n",
            "Epoch 24/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.2184 - acc: 0.9207WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 582ms/step - loss: 0.2185 - acc: 0.9207 - val_loss: 3.6127 - val_acc: 0.2926\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.30334\n",
            "Epoch 25/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.2128 - acc: 0.9241WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 580ms/step - loss: 0.2129 - acc: 0.9241 - val_loss: 3.2171 - val_acc: 0.3335\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.30334 to 0.33351, saving model to transferlearning_weights.hdf5\n",
            "Epoch 26/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1829 - acc: 0.9368WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 578ms/step - loss: 0.1829 - acc: 0.9368 - val_loss: 4.0284 - val_acc: 0.2990\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.33351\n",
            "Epoch 27/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1999 - acc: 0.9294WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 581ms/step - loss: 0.1999 - acc: 0.9295 - val_loss: 3.6262 - val_acc: 0.3254\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.33351\n",
            "Epoch 28/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1737 - acc: 0.9421WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 583ms/step - loss: 0.1738 - acc: 0.9420 - val_loss: 5.3641 - val_acc: 0.2742\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.33351\n",
            "Epoch 29/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1577 - acc: 0.9441WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 582ms/step - loss: 0.1577 - acc: 0.9441 - val_loss: 5.2199 - val_acc: 0.2839\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.33351\n",
            "Epoch 30/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1417 - acc: 0.9469WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 577ms/step - loss: 0.1419 - acc: 0.9468 - val_loss: 4.7626 - val_acc: 0.2893\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.33351\n",
            "Epoch 31/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1495 - acc: 0.9480WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 580ms/step - loss: 0.1497 - acc: 0.9480 - val_loss: 4.9404 - val_acc: 0.3157\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.33351\n",
            "Epoch 32/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1614 - acc: 0.9516WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 581ms/step - loss: 0.1614 - acc: 0.9516 - val_loss: 4.2511 - val_acc: 0.3136\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.33351\n",
            "Epoch 33/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1395 - acc: 0.9501WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 583ms/step - loss: 0.1395 - acc: 0.9501 - val_loss: 5.0492 - val_acc: 0.2904\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.33351\n",
            "Epoch 34/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1424 - acc: 0.9493WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 578ms/step - loss: 0.1424 - acc: 0.9493 - val_loss: 5.3950 - val_acc: 0.2839\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.33351\n",
            "Epoch 35/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1475 - acc: 0.9467WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 583ms/step - loss: 0.1475 - acc: 0.9467 - val_loss: 5.5044 - val_acc: 0.2408\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.33351\n",
            "Epoch 36/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1208 - acc: 0.9595WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 583ms/step - loss: 0.1208 - acc: 0.9595 - val_loss: 6.1155 - val_acc: 0.2705\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.33351\n",
            "Epoch 37/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1188 - acc: 0.9561WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 585ms/step - loss: 0.1189 - acc: 0.9561 - val_loss: 4.3329 - val_acc: 0.3120\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.33351\n",
            "Epoch 38/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1076 - acc: 0.9629WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 582ms/step - loss: 0.1077 - acc: 0.9628 - val_loss: 4.2352 - val_acc: 0.3184\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.33351\n",
            "Epoch 39/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1151 - acc: 0.9591WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 577ms/step - loss: 0.1151 - acc: 0.9591 - val_loss: 4.9096 - val_acc: 0.3012\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.33351\n",
            "Epoch 40/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1161 - acc: 0.9638WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 579ms/step - loss: 0.1161 - acc: 0.9638 - val_loss: 5.9655 - val_acc: 0.2963\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.33351\n",
            "Epoch 41/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.0974 - acc: 0.9668WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 582ms/step - loss: 0.0975 - acc: 0.9667 - val_loss: 5.6911 - val_acc: 0.3308\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.33351\n",
            "Epoch 42/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1135 - acc: 0.9571WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 581ms/step - loss: 0.1135 - acc: 0.9571 - val_loss: 5.1873 - val_acc: 0.3292\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.33351\n",
            "Epoch 43/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.0965 - acc: 0.9641WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 576ms/step - loss: 0.0965 - acc: 0.9641 - val_loss: 5.3380 - val_acc: 0.3211\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.33351\n",
            "Epoch 44/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.0939 - acc: 0.9625WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 575ms/step - loss: 0.0939 - acc: 0.9625 - val_loss: 5.2303 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.33351\n",
            "Epoch 45/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1029 - acc: 0.9645WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 581ms/step - loss: 0.1030 - acc: 0.9645 - val_loss: 5.9992 - val_acc: 0.2899\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.33351\n",
            "Epoch 46/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.0778 - acc: 0.9717WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 575ms/step - loss: 0.0778 - acc: 0.9716 - val_loss: 5.6682 - val_acc: 0.2796\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.33351\n",
            "Epoch 47/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.0959 - acc: 0.9643WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 582ms/step - loss: 0.0960 - acc: 0.9643 - val_loss: 5.6462 - val_acc: 0.3066\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.33351\n",
            "Epoch 48/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.1056 - acc: 0.9605WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 577ms/step - loss: 0.1056 - acc: 0.9605 - val_loss: 5.7278 - val_acc: 0.3141\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.33351\n",
            "Epoch 49/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.0933 - acc: 0.9691WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 101s 571ms/step - loss: 0.0933 - acc: 0.9691 - val_loss: 7.0779 - val_acc: 0.2883\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.33351\n",
            "Epoch 50/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.0843 - acc: 0.9693WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 101s 574ms/step - loss: 0.0843 - acc: 0.9693 - val_loss: 7.6908 - val_acc: 0.2866\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.33351\n",
            "Epoch 51/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.0836 - acc: 0.9719WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 581ms/step - loss: 0.0836 - acc: 0.9719 - val_loss: 7.2480 - val_acc: 0.2899\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.33351\n",
            "Epoch 52/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.0950 - acc: 0.9652WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 578ms/step - loss: 0.0950 - acc: 0.9652 - val_loss: 6.8223 - val_acc: 0.2931\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.33351\n",
            "Epoch 53/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.0809 - acc: 0.9708WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 103s 580ms/step - loss: 0.0810 - acc: 0.9708 - val_loss: 6.9619 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.33351\n",
            "Epoch 54/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - ETA: 0s - loss: 0.0746 - acc: 0.9729WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "174/174 [==============================] - 102s 579ms/step - loss: 0.0747 - acc: 0.9729 - val_loss: 7.0335 - val_acc: 0.2985\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.33351\n",
            "Epoch 55/150\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            " 36/174 [=====>........................] - ETA: 1:01 - loss: 0.0811 - acc: 0.9676"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concrete-house"
      },
      "source": [
        "### Salvando o modelo"
      ],
      "id": "concrete-house"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "distant-harassment"
      },
      "source": [
        "# carregando o melhor peso obtido para o modelo\n",
        "best_model = model\n",
        "best_model.load_weights('/content/transferlearning_weights.hdf5')"
      ],
      "id": "distant-harassment",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "humanitarian-ultimate"
      },
      "source": [
        "# salvando os dois modelos obtidos durante o treinamento\n",
        "model.save('model1')\n",
        "best_model.save('model2')"
      ],
      "id": "humanitarian-ultimate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T--QqT_wP9D9"
      },
      "source": [
        "!mv /content/model1 /content/drive/MyDrive/experimentos/v2.0-exp5-ds4\n",
        "!mv /content/model2 /content/drive/MyDrive/experimentos/v2.0-exp5-ds4"
      ],
      "id": "T--QqT_wP9D9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "professional-freight"
      },
      "source": [
        "# carregando o melhor modelo para realização de testes de desempenho\n",
        "#model = tf.keras.models.load_model('/content/drive/MyDrive/experimentos/v2.0-exp3-ds4/model2')"
      ],
      "id": "professional-freight",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broke-learning"
      },
      "source": [
        "model.evaluate(test_generator)"
      ],
      "id": "broke-learning",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jewish-saturn"
      },
      "source": [
        "### Métricas de avaliação do modelo"
      ],
      "id": "jewish-saturn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "therapeutic-medium"
      },
      "source": [
        "# carregando os dados de teste\n",
        "for i in range(0, 42):\n",
        "  (x1, y1) = test_generator[i]\n",
        "  if i == 0:\n",
        "    x, y = x1, y1\n",
        "  else:\n",
        "    x = np.concatenate((x, x1))\n",
        "    y = np.concatenate((y, y1))"
      ],
      "id": "therapeutic-medium",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dependent-gnome"
      },
      "source": [
        "# realizando a predição para os dados de teste\n",
        "predict = model.predict(x)"
      ],
      "id": "dependent-gnome",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "precise-kuwait"
      },
      "source": [
        "count_global = 0\n",
        "for predicts in predict:\n",
        "    count = 0\n",
        "    aux = np.zeros((11,))\n",
        "    for values in predicts:\n",
        "        if values >= 0.50:\n",
        "            aux[count] = 1.\n",
        "        else:\n",
        "            aux[count] = 0.\n",
        "        count += 1\n",
        "    predict[count_global] = aux\n",
        "    count_global += 1"
      ],
      "id": "precise-kuwait",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "starting-swaziland"
      },
      "source": [
        "print('Matriz de Confusão:\\n', confusion_matrix(y.argmax(axis = 1), predict.argmax(axis = 1)))\n",
        "print('Acurácia:', accuracy_score(y.argmax(axis = 1), predict.argmax(axis = 1)))\n",
        "print('Precisão', precision_score(y.argmax(axis = 1), predict.argmax(axis = 1)))\n",
        "print('Sensibilidade:', recall_score(y.argmax(axis = 1), predict.argmax(axis = 1))) \n",
        "print('F1_Score:', f1_score(y.argmax(axis = 1), predict.argmax(axis = 1)))"
      ],
      "id": "starting-swaziland",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nominated-aviation"
      },
      "source": [
        "# visualizando o ganho de acurácia durante o treinamento\n",
        "plt.figure(figsize = (15,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig('model-accuracy')\n",
        "\n",
        "# visualizando o decaimento da função de custo durante o treinamento \n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig('model-loss')"
      ],
      "id": "nominated-aviation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upset-sending"
      },
      "source": [
        "### Visualizando a arquitetura da rede"
      ],
      "id": "upset-sending"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compatible-contract"
      },
      "source": [
        "# visualizando a arquitetura do modelo\n",
        "model.summary()"
      ],
      "id": "compatible-contract",
      "execution_count": null,
      "outputs": []
    }
  ]
}