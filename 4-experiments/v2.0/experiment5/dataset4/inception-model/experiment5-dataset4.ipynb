{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "colab": {
      "name": "experiment5-dataset4.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weekly-aerospace"
      },
      "source": [
        "# Experimento 5 \n",
        "***\n",
        "- Rede utilizada: Inception\n",
        "- Conjunto de Dados: VinBigData\n",
        "- Modelando uma para classificar problemas pulmonares em 11 classes distintas"
      ],
      "id": "weekly-aerospace"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "current-following"
      },
      "source": [
        "### Importação dos pacotes"
      ],
      "id": "current-following"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meaningful-iraqi"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "meaningful-iraqi",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "banned-comfort"
      },
      "source": [
        "### Pré-processamento nos dados"
      ],
      "id": "banned-comfort"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heated-adult"
      },
      "source": [
        "# lendo os dados de um arquivo csv\n",
        "dataframe = pd.read_csv('/content/drive/MyDrive/vinbigdata/train.csv')\n",
        "# criando uma coluna com os caminhos relativos as imagens\n",
        "dataframe['image_path'] = '/content/drive/MyDrive/vinbigdata/train/' + dataframe.image_id + '.jpg'"
      ],
      "id": "heated-adult",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "starting-setting",
        "outputId": "38c39128-dbfe-4dd8-cb4d-ed6a51ce181b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('total de imagens disponíveis:', str(len(set(dataframe['image_path']))))"
      ],
      "id": "starting-setting",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total de imagens disponíveis: 15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tough-newfoundland",
        "outputId": "58304880-1fda-4757-8d85-62b4c0b4d475",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# visualizando os casos disponíveis\n",
        "dataframe['class_name'].value_counts()"
      ],
      "id": "tough-newfoundland",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "No finding            31818\n",
              "Aortic enlargement     7162\n",
              "Cardiomegaly           5427\n",
              "Pleural thickening     4842\n",
              "Pulmonary fibrosis     4655\n",
              "Nodule/Mass            2580\n",
              "Lung Opacity           2483\n",
              "Pleural effusion       2476\n",
              "Other lesion           2203\n",
              "Infiltration           1247\n",
              "ILD                    1000\n",
              "Calcification           960\n",
              "Consolidation           556\n",
              "Atelectasis             279\n",
              "Pneumothorax            226\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "intermediate-anchor"
      },
      "source": [
        "# removendo os casos não relativos a distúrbios pulmonares\n",
        "dataframe = dataframe[dataframe.class_name != 'Aortic enlargement']\n",
        "dataframe = dataframe[dataframe.class_name != 'Cardiomegaly']\n",
        "dataframe = dataframe[dataframe.class_name != 'Other lesion']"
      ],
      "id": "intermediate-anchor",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "retained-message",
        "outputId": "be83c31c-2fb6-4bee-a98d-3f8a9a2b0e0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# separando os casos rotulados como normais e anormais\n",
        "normal_cases = dataframe[(dataframe.class_id == 14) & (dataframe.class_name == 'No finding')]\n",
        "abnormal_cases = dataframe[(dataframe.class_id != 14) & (dataframe.class_name != 'No finding')]\n",
        "\n",
        "print('total de dados após a filtração:', str(len(set(normal_cases['image_path'])) + len(set(abnormal_cases['image_path']))))"
      ],
      "id": "retained-message",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total de dados após a filtração: 13952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forty-system"
      },
      "source": [
        "# removendo as imagens repetidas\n",
        "normal_data = normal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
        "abnormal_data = abnormal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
        "\n",
        "# criando dataframes especifos com caminhos para as imagens e rótulos\n",
        "normal_data['target'] = 'normal'\n",
        "abnormal_data['target'] = 'abnormal'"
      ],
      "id": "forty-system",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viral-conspiracy",
        "outputId": "d3ea3068-ef06-4dbd-dbb4-d82a1f8a4bc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('quantidade de dados rotulados como normais:', len(normal_data))\n",
        "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
      ],
      "id": "viral-conspiracy",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de dados rotulados como normais: 10606\n",
            "quantidade de dados rotulados como anormais: 3346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baking-shield"
      },
      "source": [
        "# removendo 69% dos casos normais para balancear os dados\n",
        "normal, _ = train_test_split(normal_data, test_size = 0.69, random_state = 42)"
      ],
      "id": "baking-shield",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "little-communications",
        "outputId": "4063ed91-c7ef-4ef2-bc68-866bb9bc241d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('quantidade de dados rotulados como normais:', len(normal))\n",
        "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
      ],
      "id": "little-communications",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de dados rotulados como normais: 3287\n",
            "quantidade de dados rotulados como anormais: 3346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flying-supervision"
      },
      "source": [
        "# concatenando os dataframes de casos normais e anormais\n",
        "full_data = abnormal_data"
      ],
      "id": "flying-supervision",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRS6Rb4Oj-t6",
        "outputId": "58ddd29e-9c97-433c-e760-6d90d040e67f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# visualizando a quantidade de exemplos por classe disponíveis\n",
        "full_data['class_name'].value_counts()"
      ],
      "id": "QRS6Rb4Oj-t6",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pleural thickening    901\n",
              "Pulmonary fibrosis    742\n",
              "Lung Opacity          427\n",
              "Nodule/Mass           339\n",
              "Pleural effusion      328\n",
              "Calcification         167\n",
              "Infiltration          163\n",
              "ILD                   152\n",
              "Consolidation          59\n",
              "Atelectasis            41\n",
              "Pneumothorax           27\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worst-management"
      },
      "source": [
        "# separando os dados de treinamento e de teste\n",
        "train_df, test_df = train_test_split(full_data, stratify = full_data['target'],\n",
        "                                     test_size = 0.2, random_state = 42)"
      ],
      "id": "worst-management",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "future-weather"
      },
      "source": [
        "# separando os dados de validação dos dados de treinamento\n",
        "train_df, validation_df = train_test_split(train_df, stratify = train_df['target'],\n",
        "                                           test_size = 0.2, random_state = 42)"
      ],
      "id": "future-weather",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secret-session",
        "outputId": "cdd17670-72e6-466d-bf56-918741a3fcfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# visualizando a quantidade de dados\n",
        "print('quantidade de imagens de treinamento:', len(train_df['image_path']))\n",
        "print('quantidade de rótulos de treinamento:', len(train_df['target']))\n",
        "print('quantidade de imagens de teste:', len(test_df['image_path']))\n",
        "print('quantidade de rótulos de teste:', len(test_df['target']))\n",
        "print('quantidade de imagens de validação:', len(validation_df['image_path']))\n",
        "print('quantidade de rótulos de validação:', len(validation_df['target']))"
      ],
      "id": "secret-session",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de imagens de treinamento: 2140\n",
            "quantidade de rótulos de treinamento: 2140\n",
            "quantidade de imagens de teste: 670\n",
            "quantidade de rótulos de teste: 670\n",
            "quantidade de imagens de validação: 536\n",
            "quantidade de rótulos de validação: 536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bulgarian-staff",
        "outputId": "aa035e4f-c695-453c-dfb0-99da61434092",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# normalizando as imagens de treinamento e aplicando aumento de dados\n",
        "image_generator = ImageDataGenerator(rescale = 1./255.,\n",
        "                                     rotation_range = 10, zoom_range = 0.2)\n",
        "\n",
        "# criando o gerador de imagens de treinamento \n",
        "train_generator = image_generator.flow_from_dataframe(\n",
        "                                                      dataframe = train_df,\n",
        "                                                      directory = '',\n",
        "                                                      x_col = 'image_path',\n",
        "                                                      y_col = 'class_name',\n",
        "                                                      batch_size = 32,\n",
        "                                                      seed = 42,\n",
        "                                                      shuffle = True,\n",
        "                                                      class_mode = 'categorical',\n",
        "                                                      target_size = (256, 256))\n",
        "\n",
        "# normalizando as imagens de teste e validação\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255.)\n",
        "\n",
        "# criando o gerador de imagens de validação \n",
        "valid_generator = test_datagen.flow_from_dataframe(\n",
        "                                                      dataframe = validation_df,\n",
        "                                                      directory = '.', \n",
        "                                                      x_col = 'image_path',\n",
        "                                                      y_col = 'class_name',\n",
        "                                                      batch_size = 32,\n",
        "                                                      seed = 42,\n",
        "                                                      shuffle = True,\n",
        "                                                      class_mode = 'categorical',\n",
        "                                                      target_size = (256, 256))\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "                                                  dataframe = test_df, \n",
        "                                                  directory = '.',\n",
        "                                                  x_col = 'image_path',\n",
        "                                                  y_col = 'class_name',\n",
        "                                                  batch_size = 32,\n",
        "                                                  seed = 42,\n",
        "                                                  shuffle = True,\n",
        "                                                  class_mode = 'categorical',\n",
        "                                                  target_size = (256, 256))"
      ],
      "id": "bulgarian-staff",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2140 validated image filenames belonging to 11 classes.\n",
            "Found 536 validated image filenames belonging to 11 classes.\n",
            "Found 670 validated image filenames belonging to 11 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rolled-luxury"
      },
      "source": [
        "### Preparando a rede neural convolucional"
      ],
      "id": "rolled-luxury"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "considered-arnold",
        "outputId": "7694ea9e-cc1f-42ba-ebf4-aec76b1b422c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# baixando os pesos treinados da rede inception\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "id": "considered-arnold",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-22 04:11:52--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.127.128, 172.217.218.128, 74.125.128.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.127.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/tmp/inception_v3_w 100%[===================>]  83.84M  91.2MB/s    in 0.9s    \n",
            "\n",
            "2021-04-22 04:11:53 (91.2 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "promotional-tooth"
      },
      "source": [
        "# referenciando o local em que os pesos estão armazenados\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "# carregando a arquitetura inception pré-treinada\n",
        "pre_trained_model = InceptionV3(input_shape = (256, 256, 3), \n",
        "                                include_top = False, \n",
        "                                weights = None)\n",
        "\n",
        "# carregando os pesos treinados com outros dados \n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "# definindo as flags iniciais  \n",
        "pre_trained_model.trainable = True\n",
        "set_trainable = False\n",
        "\n",
        "# para a arquitetura inception, a rede será retreinada a partir da camada 'mixed8'\n",
        "for layer in pre_trained_model.layers:\n",
        "    if layer.name == 'mixed8':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "# obtendo a última camada como sendo a nomeada por 'mixed7'\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "last_output = last_layer.output"
      ],
      "id": "promotional-tooth",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worst-establishment"
      },
      "source": [
        "x = layers.Flatten()(last_output)\n",
        "# adicionando uma camada densa com 512 neurônios\n",
        "x = layers.Dense(units = 512, activation = tf.nn.relu)(x)     \n",
        "# conecatando a rede uma camada com 128 neurônios e função de ativação relu\n",
        "x = layers.Dense(units = 256, activation = tf.nn.relu)(x) \n",
        "# aplicando uma camada de dropout com uma taxa de 20% (normalização)\n",
        "x = layers.Dropout(rate = 0.2)(x)      \n",
        "# adicionando uma camada de saída com um neurônio e uma função de ativação sigmoide\n",
        "x = layers.Dense  (units = 2, activation = tf.nn.softmax)(x)      \n",
        "\n",
        "# conecatando as camadas definidas acima com a arquitetura inception\n",
        "model = Model(pre_trained_model.input, x) \n",
        "\n",
        "# compilando a rede \n",
        "model.compile(optimizer = optimizers.RMSprop(learning_rate = 0.0001), loss = 'categorical_crossentropy', \n",
        "              metrics = ['acc'])"
      ],
      "id": "worst-establishment",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UpprXhUldhX"
      },
      "source": [
        "# realizando a transferência de aprendizado com os pesos do experimento 3\n",
        "model.load_weights('/content/drive/MyDrive/experimentos/v2.0-exp3-ds4/transferlearning_weights.hdf5', )"
      ],
      "id": "-UpprXhUldhX",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x5vKjajmBdB"
      },
      "source": [
        "# obtendo a última camada como sendo a nomeada por 'mixed7'\n",
        "last_layer = model.get_layer('dropout')\n",
        "last_output = last_layer.output\n",
        "\n",
        "# adicionando uma camada de saída com um neurônio e uma função de ativação sigmoide\n",
        "x = layers.Dense(units = 11, activation = tf.nn.softmax)(last_output)      \n",
        "\n",
        "# conecatando as camadas definidas acima com a arquitetura inception\n",
        "model = Model(pre_trained_model.input, x) \n",
        "\n",
        "# compilando a rede \n",
        "model.compile(optimizer = optimizers.RMSprop(learning_rate = 0.0001), loss = 'categorical_crossentropy', \n",
        "              metrics = ['acc'])"
      ],
      "id": "4x5vKjajmBdB",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "engaged-culture"
      },
      "source": [
        "# definindo o caminho pelo qual os pesos serão armazenados \n",
        "filepath = \"transferlearning_weights.hdf5\"\n",
        "# callback para salvar o melhor valor dos pesos em relação ao desempenho com os dados de validação \n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max')\n",
        "\n",
        "lr_reduce = ReduceLROnPlateau(monitor = 'val_acc', factor=0.1, min_delta = 0.00001, patience = 5, verbose = 1)\n",
        "# definindo um array de callbacks\n",
        "callbacks = [checkpoint, lr_reduce]"
      ],
      "id": "engaged-culture",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seven-stock"
      },
      "source": [
        "# definindo as flags iniciais  \n",
        "model.trainable = True\n",
        "set_trainable = False\n",
        "\n",
        "# para a arquitetura inception, a rede será retreinada a partir da camada 'mixed8'\n",
        "for layer in model.layers:\n",
        "    if layer.name == 'mixed5':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ],
      "id": "seven-stock",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mighty-manufacturer",
        "outputId": "f12afa9e-9abd-49eb-9e85-b8d2dfc215df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# treinando a rede neural convolucional\n",
        "history = model.fit_generator(train_generator, steps_per_epoch = 2140 // 32, \n",
        "                              validation_data = valid_generator, validation_steps = 536 // 32,\n",
        "                              callbacks = callbacks, epochs = 20, use_multiprocessing = True,\n",
        "                              workers = 8)"
      ],
      "id": "mighty-manufacturer",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "65/66 [============================>.] - ETA: 0s - loss: 2.6924 - acc: 0.2081WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 86s 1s/step - loss: 2.6816 - acc: 0.2089 - val_loss: 2.1027 - val_acc: 0.3066\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.30664, saving model to transferlearning_weights.hdf5\n",
            "Epoch 2/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 2.0345 - acc: 0.2814WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 81s 1s/step - loss: 2.0343 - acc: 0.2814 - val_loss: 2.0575 - val_acc: 0.2734\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.30664\n",
            "Epoch 3/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "65/66 [============================>.] - ETA: 0s - loss: 1.9103 - acc: 0.3218WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 82s 1s/step - loss: 1.9101 - acc: 0.3216 - val_loss: 2.0814 - val_acc: 0.2988\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.30664\n",
            "Epoch 4/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.8274 - acc: 0.3421WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 81s 1s/step - loss: 1.8275 - acc: 0.3421 - val_loss: 2.0554 - val_acc: 0.2871\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.30664\n",
            "Epoch 5/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.7584 - acc: 0.3737WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 81s 1s/step - loss: 1.7582 - acc: 0.3737 - val_loss: 1.9787 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.30664 to 0.31250, saving model to transferlearning_weights.hdf5\n",
            "Epoch 6/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.6530 - acc: 0.3957WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 77s 1s/step - loss: 1.6530 - acc: 0.3957 - val_loss: 2.0953 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.31250\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.5678 - acc: 0.4326WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 77s 1s/step - loss: 1.5679 - acc: 0.4328 - val_loss: 2.0342 - val_acc: 0.2676\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.31250\n",
            "Epoch 8/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.4805 - acc: 0.4786WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 78s 1s/step - loss: 1.4808 - acc: 0.4784 - val_loss: 2.0866 - val_acc: 0.2598\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.31250\n",
            "Epoch 9/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.3782 - acc: 0.5190WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 76s 1s/step - loss: 1.3784 - acc: 0.5190 - val_loss: 2.0201 - val_acc: 0.2910\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.31250\n",
            "Epoch 10/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.2856 - acc: 0.5488WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 75s 1s/step - loss: 1.2860 - acc: 0.5487 - val_loss: 2.0960 - val_acc: 0.3066\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.31250\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "Epoch 11/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.1365 - acc: 0.6319WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 76s 1s/step - loss: 1.1365 - acc: 0.6320 - val_loss: 2.0759 - val_acc: 0.2949\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.31250\n",
            "Epoch 12/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.0722 - acc: 0.6704WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 75s 1s/step - loss: 1.0722 - acc: 0.6704 - val_loss: 2.0819 - val_acc: 0.2891\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.31250\n",
            "Epoch 13/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.0886 - acc: 0.6537WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 76s 1s/step - loss: 1.0883 - acc: 0.6538 - val_loss: 2.1185 - val_acc: 0.2754\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.31250\n",
            "Epoch 14/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "65/66 [============================>.] - ETA: 0s - loss: 1.0488 - acc: 0.6439WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 77s 1s/step - loss: 1.0488 - acc: 0.6442 - val_loss: 2.0982 - val_acc: 0.2695\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.31250\n",
            "Epoch 15/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.0368 - acc: 0.6589WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 78s 1s/step - loss: 1.0368 - acc: 0.6591 - val_loss: 2.0716 - val_acc: 0.2734\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.31250\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "Epoch 16/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.9991 - acc: 0.6973WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 76s 1s/step - loss: 0.9992 - acc: 0.6971 - val_loss: 2.0736 - val_acc: 0.2871\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.31250\n",
            "Epoch 17/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.0461 - acc: 0.6606WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 77s 1s/step - loss: 1.0459 - acc: 0.6606 - val_loss: 2.1082 - val_acc: 0.2656\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.31250\n",
            "Epoch 18/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.9732 - acc: 0.7175WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 75s 1s/step - loss: 0.9736 - acc: 0.7171 - val_loss: 2.0795 - val_acc: 0.2754\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.31250\n",
            "Epoch 19/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.0267 - acc: 0.6714WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 76s 1s/step - loss: 1.0266 - acc: 0.6715 - val_loss: 2.0831 - val_acc: 0.2754\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.31250\n",
            "Epoch 20/20\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 1.0045 - acc: 0.6907WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 77s 1s/step - loss: 1.0044 - acc: 0.6907 - val_loss: 2.0633 - val_acc: 0.2793\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.31250\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concrete-house"
      },
      "source": [
        "### Salvando o modelo"
      ],
      "id": "concrete-house"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "distant-harassment"
      },
      "source": [
        "# carregando o melhor peso obtido para o modelo\n",
        "best_model = model\n",
        "best_model.load_weights('/content/transferlearning_weights.hdf5')"
      ],
      "id": "distant-harassment",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "humanitarian-ultimate",
        "outputId": "4abced5d-7cbb-45f3-9927-0e0eac529498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# salvando os dois modelos obtidos durante o treinamento\n",
        "model.save('model1')\n",
        "best_model.save('model2')"
      ],
      "id": "humanitarian-ultimate",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "INFO:tensorflow:Assets written to: model2/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "professional-freight"
      },
      "source": [
        "# carregando o melhor modelo para realização de testes de desempenho\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/experimentos/v2.0-exp3-ds4/model2')"
      ],
      "id": "professional-freight",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broke-learning",
        "outputId": "b67895eb-5689-40e2-e0e6-5a494998c236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "model.evaluate(test_generator)"
      ],
      "id": "broke-learning",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-52f55913fcee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 895\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[32,2] labels_size=[32,11]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits (defined at <ipython-input-28-52f55913fcee>:1) ]] [Op:__inference_test_function_131131]\n\nFunction call stack:\ntest_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jewish-saturn"
      },
      "source": [
        "### Métricas de avaliação do modelo"
      ],
      "id": "jewish-saturn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "therapeutic-medium"
      },
      "source": [
        "# carregando os dados de teste\n",
        "for i in range(0, 42):\n",
        "  (x1, y1) = test_generator[i]\n",
        "  if i == 0:\n",
        "    x, y = x1, y1\n",
        "  else:\n",
        "    x = np.concatenate((x, x1))\n",
        "    y = np.concatenate((y, y1))"
      ],
      "id": "therapeutic-medium",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dependent-gnome"
      },
      "source": [
        "# realizando a predição para os dados de teste\n",
        "predict = model.predict(x)"
      ],
      "id": "dependent-gnome",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "precise-kuwait"
      },
      "source": [
        "count_global = 0\n",
        "for predicts in predict:\n",
        "    count = 0\n",
        "    aux = np.zeros((11,))\n",
        "    for values in predicts:\n",
        "        if values >= 0.50:\n",
        "            aux[count] = 1.\n",
        "        else:\n",
        "            aux[count] = 0.\n",
        "        count += 1\n",
        "    predict[count_global] = aux\n",
        "    count_global += 1"
      ],
      "id": "precise-kuwait",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "starting-swaziland"
      },
      "source": [
        "print('Matriz de Confusão:\\n', confusion_matrix(y.argmax(axis = 1), predict.argmax(axis = 1)))\n",
        "print('Acurácia:', accuracy_score(y.argmax(axis = 1), predict.argmax(axis = 1)))\n",
        "print('Precisão', precision_score(y.argmax(axis = 1), predict.argmax(axis = 1)))\n",
        "print('Sensibilidade:', recall_score(y.argmax(axis = 1), predict.argmax(axis = 1))) \n",
        "print('F1_Score:', f1_score(y.argmax(axis = 1), predict.argmax(axis = 1)))"
      ],
      "id": "starting-swaziland",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nominated-aviation"
      },
      "source": [
        "# visualizando o ganho de acurácia durante o treinamento\n",
        "plt.figure(figsize = (15,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig('model-accuracy')\n",
        "\n",
        "# visualizando o decaimento da função de custo durante o treinamento \n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig('model-loss')"
      ],
      "id": "nominated-aviation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upset-sending"
      },
      "source": [
        "### Visualizando a arquitetura da rede"
      ],
      "id": "upset-sending"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compatible-contract"
      },
      "source": [
        "# visualizando a arquitetura do modelo\n",
        "model.summary()"
      ],
      "id": "compatible-contract",
      "execution_count": null,
      "outputs": []
    }
  ]
}