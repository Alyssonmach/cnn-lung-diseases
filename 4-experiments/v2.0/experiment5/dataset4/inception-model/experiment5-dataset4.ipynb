{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weekly-aerospace",
   "metadata": {},
   "source": [
    "# Experimento 5 \n",
    "***\n",
    "- Rede utilizada: Inception\n",
    "- Conjunto de Dados: VinBigData\n",
    "- Modelando uma para classificar problemas pulmonares em 11 classes distintas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-following",
   "metadata": {},
   "source": [
    "### Importação dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-comfort",
   "metadata": {},
   "source": [
    "### Pré-processamento nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo os dados de um arquivo csv\n",
    "dataframe = pd.read_csv('/content/drive/MyDrive/vinbigdata/train.csv')\n",
    "# criando uma coluna com os caminhos relativos as imagens\n",
    "dataframe['image_path'] = '/content/drive/MyDrive/vinbigdata/train/' + dataframe.image_id + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total de imagens disponíveis:', str(len(set(dataframe['image_path']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando os casos disponíveis\n",
    "dataframe['class_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo os casos não relativos a distúrbios pulmonares\n",
    "dataframe = dataframe[dataframe.class_name != 'Aortic enlargement']\n",
    "dataframe = dataframe[dataframe.class_name != 'Cardiomegaly']\n",
    "dataframe = dataframe[dataframe.class_name != 'Other lesion']\n",
    "dataframe = dataframe[dataframe.class_name != 'Consolidation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando os casos rotulados como normais e anormais\n",
    "normal_cases = dataframe[(dataframe.class_id == 14) & (dataframe.class_name == 'No finding')]\n",
    "abnormal_cases = dataframe[(dataframe.class_id != 14) & (dataframe.class_name != 'No finding')]\n",
    "\n",
    "print('total de dados após a filtração:', str(len(set(normal_cases['image_path'])) + len(set(abnormal_cases['image_path']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo as imagens repetidas\n",
    "normal_data = normal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
    "abnormal_data = abnormal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
    "\n",
    "# criando dataframes especifos com caminhos para as imagens e rótulos\n",
    "normal_data['target'] = 'normal'\n",
    "abnormal_data['target'] = 'abnormal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('quantidade de dados rotulados como normais:', len(normal_data))\n",
    "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo 69% dos casos normais para balancear os dados\n",
    "normal, _ = train_test_split(normal_data, test_size = 0.69, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('quantidade de dados rotulados como normais:', len(normal))\n",
    "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenando os dataframes de casos normais e anormais\n",
    "full_data = pd.concat([normal, abnormal_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# misturando todos os dados do dataframe e reiniciando os valores dos índices \n",
    "full_data = full_data.sample(frac = 1, axis = 0, random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando os dados de treinamento e de teste\n",
    "train_df, test_df = train_test_split(full_data, stratify = full_data['target'],\n",
    "                                     test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando os dados de validação dos dados de treinamento\n",
    "train_df, validation_df = train_test_split(train_df, stratify = train_df['target'],\n",
    "                                           test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando a quantidade de dados\n",
    "print('quantidade de imagens de treinamento:', len(train_df['image_path']))\n",
    "print('quantidade de rótulos de treinamento:', len(train_df['target']))\n",
    "print('quantidade de imagens de teste:', len(test_df['image_path']))\n",
    "print('quantidade de rótulos de teste:', len(test_df['target']))\n",
    "print('quantidade de imagens de validação:', len(validation_df['image_path']))\n",
    "print('quantidade de rótulos de validação:', len(validation_df['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizando as imagens de treinamento e aplicando aumento de dados\n",
    "image_generator = ImageDataGenerator(rescale = 1./255.,\n",
    "                                     rotation_range = 10, zoom_range = 0.2)\n",
    "\n",
    "# criando o gerador de imagens de treinamento \n",
    "train_generator = image_generator.flow_from_dataframe(\n",
    "                                                      dataframe = train_df,\n",
    "                                                      directory = '',\n",
    "                                                      x_col = 'image_path',\n",
    "                                                      y_col = 'target',\n",
    "                                                      batch_size = 32,\n",
    "                                                      seed = 42,\n",
    "                                                      shuffle = True,\n",
    "                                                      class_mode = 'categorical',\n",
    "                                                      target_size = (256, 256))\n",
    "\n",
    "# normalizando as imagens de teste e validação\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "# criando o gerador de imagens de validação \n",
    "valid_generator = test_datagen.flow_from_dataframe(\n",
    "                                                      dataframe = validation_df,\n",
    "                                                      directory = '.', \n",
    "                                                      x_col = 'image_path',\n",
    "                                                      y_col = 'target',\n",
    "                                                      batch_size = 32,\n",
    "                                                      seed = 42,\n",
    "                                                      shuffle = True,\n",
    "                                                      class_mode = 'categorical',\n",
    "                                                      target_size = (256, 256))\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "                                                  dataframe = test_df, \n",
    "                                                  directory = '.',\n",
    "                                                  x_col = 'image_path',\n",
    "                                                  y_col = 'target',\n",
    "                                                  batch_size = 32,\n",
    "                                                  seed = 42,\n",
    "                                                  shuffle = True,\n",
    "                                                  class_mode = 'categorical',\n",
    "                                                  target_size = (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-luxury",
   "metadata": {},
   "source": [
    "### Preparando a rede neural convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baixando os pesos treinados da rede inception\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
    "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenciando o local em que os pesos estão armazenados\n",
    "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "# carregando a arquitetura inception pré-treinada\n",
    "pre_trained_model = InceptionV3(input_shape = (256, 256, 3), \n",
    "                                include_top = False, \n",
    "                                weights = None)\n",
    "\n",
    "# carregando os pesos treinados com outros dados \n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "# definindo as flags iniciais  \n",
    "pre_trained_model.trainable = True\n",
    "set_trainable = False\n",
    "\n",
    "# para a arquitetura inception, a rede será retreinada a partir da camada 'mixed8'\n",
    "for layer in pre_trained_model.layers:\n",
    "    if layer.name == 'mixed8':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "# obtendo a última camada como sendo a nomeada por 'mixed7'\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.GlobalAveragePooling2D()(last_output)\n",
    "# adicionando uma camada densa com 512 neurônios\n",
    "x = layers.Dense(units = 512, activation = tf.nn.relu)(x)     \n",
    "# conecatando a rede uma camada com 128 neurônios e função de ativação relu\n",
    "x = layers.Dense(units = 256, activation = tf.nn.relu)(x) \n",
    "# aplicando uma camada de dropout com uma taxa de 20% (normalização)\n",
    "x = layers.Dropout(rate = 0.2)(x)      \n",
    "# adicionando uma camada de saída com um neurônio e uma função de ativação sigmoide\n",
    "x = layers.Dense  (units = 2, activation = tf.nn.softmax)(x)      \n",
    "\n",
    "# conecatando as camadas definidas acima com a arquitetura inception\n",
    "model = Model(pre_trained_model.input, x) \n",
    "\n",
    "# compilando a rede \n",
    "model.compile(optimizer = optimizers.RMSprop(learning_rate = 0.0001), loss = 'categorical_crossentropy', \n",
    "              metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.load_model('/content/drive/MyDrive/experimentos/v2.0-exp1-ds1/model2')\n",
    "model2 = tf.keras.models.load_model('/content/drive/MyDrive/experimentos/v2.0-exp1-ds2/model2')\n",
    "model3 = tf.keras.models.load_model('/content/drive/MyDrive/experimentos/v2.0-exp1-ds3/model2')\n",
    "\n",
    "weights1 = model1.get_weights()\n",
    "weights2 = model2.get_weights()\n",
    "weights3 = model3.get_weights()\n",
    "\n",
    "weights1 = np.array(weights1, dtype = object)\n",
    "weights2 = np.array(weights2, dtype = object)\n",
    "weights3 = np.array(weights3, dtype = object)\n",
    "\n",
    "new_weights = (weights1 + weights2 + weights3) / 3\n",
    "\n",
    "model.set_weights(list(new_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo a última camada como sendo a nomeada por 'mixed7'\n",
    "last_layer = model.get_layer('mixed7')\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = layers.Flatten()(last_output)\n",
    "# adicionando uma camada densa com 512 neurônios\n",
    "x = layers.Dense(units = 512, activation = tf.nn.relu)(x)     \n",
    "# conecatando a rede uma camada com 128 neurônios e função de ativação relu\n",
    "x = layers.Dense(units = 256, activation = tf.nn.relu)(x) \n",
    "# aplicando uma camada de dropout com uma taxa de 20% (normalização)\n",
    "x = layers.Dropout(rate = 0.2)(x)      \n",
    "# adicionando uma camada de saída com um neurônio e uma função de ativação sigmoide\n",
    "x = layers.Dense  (units = 2, activation = tf.nn.softmax)(x)      \n",
    "\n",
    "# conecatando as camadas definidas acima com a arquitetura inception\n",
    "model = Model(pre_trained_model.input, x) \n",
    "\n",
    "# compilando a rede \n",
    "model.compile(optimizer = optimizers.RMSprop(learning_rate = 0.0001), loss = 'categorical_crossentropy', \n",
    "              metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo o caminho pelo qual os pesos serão armazenados \n",
    "filepath = \"transferlearning_weights.hdf5\"\n",
    "# callback para salvar o melhor valor dos pesos em relação ao desempenho com os dados de validação \n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max')\n",
    "# definindo um array de callbacks\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo as flags iniciais  \n",
    "model.trainable = True\n",
    "set_trainable = False\n",
    "\n",
    "# para a arquitetura inception, a rede será retreinada a partir da camada 'mixed8'\n",
    "for layer in model.layers:\n",
    "    if layer.name == 'dense_16':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinando a rede neural convolucional\n",
    "history = model.fit_generator(train_generator, steps_per_epoch = 4242 // 32, \n",
    "                              validation_data = valid_generator, validation_steps = 1061 // 32,\n",
    "                              callbacks = callbacks, epochs = 10, use_multiprocessing = True,\n",
    "                              workers = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-house",
   "metadata": {},
   "source": [
    "### Salvando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando o melhor peso obtido para o modelo\n",
    "best_model = model\n",
    "best_model.load_weights('/content/transferlearning_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando os dois modelos obtidos durante o treinamento\n",
    "model.save('model1')\n",
    "best_model.save('model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando o melhor modelo para realização de testes de desempenho\n",
    "model = tf.keras.models.load_model('/content/drive/MyDrive/experimentos/v2.0-exp3-ds4/model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-saturn",
   "metadata": {},
   "source": [
    "### Métricas de avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando os dados de teste\n",
    "for i in range(0, 42):\n",
    "  (x1, y1) = test_generator[i]\n",
    "  if i == 0:\n",
    "    x, y = x1, y1\n",
    "  else:\n",
    "    x = np.concatenate((x, x1))\n",
    "    y = np.concatenate((y, y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizando a predição para os dados de teste\n",
    "predict = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_global = 0\n",
    "for predicts in predict:\n",
    "    count = 0\n",
    "    aux = np.zeros((11,))\n",
    "    for values in predicts:\n",
    "        if values >= 0.50:\n",
    "            aux[count] = 1.\n",
    "        else:\n",
    "            aux[count] = 0.\n",
    "        count += 1\n",
    "    predict[count_global] = aux\n",
    "    count_global += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Matriz de Confusão:\\n', confusion_matrix(y.argmax(axis = 1), predict.argmax(axis = 1)))\n",
    "print('Acurácia:', accuracy_score(y.argmax(axis = 1), predict.argmax(axis = 1)))\n",
    "print('Precisão', precision_score(y.argmax(axis = 1), predict.argmax(axis = 1)))\n",
    "print('Sensibilidade:', recall_score(y.argmax(axis = 1), predict.argmax(axis = 1))) \n",
    "print('F1_Score:', f1_score(y.argmax(axis = 1), predict.argmax(axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando o ganho de acurácia durante o treinamento\n",
    "plt.figure(figsize = (15,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('model-accuracy')\n",
    "\n",
    "# visualizando o decaimento da função de custo durante o treinamento \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('model-loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-sending",
   "metadata": {},
   "source": [
    "### Visualizando a arquitetura da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando a arquitetura do modelo\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
