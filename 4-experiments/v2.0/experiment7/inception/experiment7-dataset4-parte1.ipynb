{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "colab": {
      "name": "experimento1-dataset4-parte1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demanding-medication"
      },
      "source": [
        "# Experimento 7\n",
        "***\n",
        "- Conjunto de Dados: VinBigData\n",
        "- Testando a predição por multi-classe\n",
        "- Arquitetura utilizada: Inception"
      ],
      "id": "demanding-medication"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laughing-kazakhstan"
      },
      "source": [
        "### Importação dos pacotes"
      ],
      "id": "laughing-kazakhstan"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "applicable-billion"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "applicable-billion",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worst-fisher"
      },
      "source": [
        "### Pré-processamento nos dados"
      ],
      "id": "worst-fisher"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "regulation-costume"
      },
      "source": [
        "# lendo os dados de um arquivo csv\n",
        "dataframe = pd.read_csv('/content/drive/MyDrive/vinbigdata/train.csv')\n",
        "# criando uma coluna com os caminhos relativos as imagens\n",
        "dataframe['image_path'] = '/content/drive/MyDrive/vinbigdata/train/' + dataframe.image_id + '.jpg'"
      ],
      "id": "regulation-costume",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "necessary-medication",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc6b458-d76a-47c5-f377-acfac15e64dd"
      },
      "source": [
        "print('total de imagens disponíveis:', str(len(set(dataframe['image_path']))))"
      ],
      "id": "necessary-medication",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total de imagens disponíveis: 15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "widespread-enclosure",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b406e99e-a132-4451-af3f-53e6a253b862"
      },
      "source": [
        "# visualizando os casos disponíveis\n",
        "dataframe['class_name'].value_counts()"
      ],
      "id": "widespread-enclosure",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "No finding            31818\n",
              "Aortic enlargement     7162\n",
              "Cardiomegaly           5427\n",
              "Pleural thickening     4842\n",
              "Pulmonary fibrosis     4655\n",
              "Nodule/Mass            2580\n",
              "Lung Opacity           2483\n",
              "Pleural effusion       2476\n",
              "Other lesion           2203\n",
              "Infiltration           1247\n",
              "ILD                    1000\n",
              "Calcification           960\n",
              "Consolidation           556\n",
              "Atelectasis             279\n",
              "Pneumothorax            226\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "informal-ottawa"
      },
      "source": [
        "# removendo os casos não relativos a distúrbios pulmonares\n",
        "dataframe = dataframe[dataframe.class_name != 'Aortic enlargement']\n",
        "dataframe = dataframe[dataframe.class_name != 'Cardiomegaly']\n",
        "dataframe = dataframe[dataframe.class_name != 'Other lesion']"
      ],
      "id": "informal-ottawa",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgBPCzM2cmBN",
        "outputId": "1a4426e8-4818-4488-af03-be4de1f77a28"
      },
      "source": [
        "# visualizando os casos disponíveis\n",
        "dataframe['class_name'].value_counts()"
      ],
      "id": "PgBPCzM2cmBN",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "No finding            31818\n",
              "Pleural thickening     4842\n",
              "Pulmonary fibrosis     4655\n",
              "Nodule/Mass            2580\n",
              "Lung Opacity           2483\n",
              "Pleural effusion       2476\n",
              "Infiltration           1247\n",
              "ILD                    1000\n",
              "Calcification           960\n",
              "Consolidation           556\n",
              "Atelectasis             279\n",
              "Pneumothorax            226\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plastic-master",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474c2037-8525-480b-f290-c396ab89869d"
      },
      "source": [
        "# separando os casos rotulados como normais e anormais\n",
        "normal_cases = dataframe[(dataframe.class_id == 14) & (dataframe.class_name == 'No finding')]\n",
        "abnormal_cases = dataframe[(dataframe.class_id != 14) & (dataframe.class_name != 'No finding')]\n",
        "\n",
        "print('total de dados após a filtração:', str(len(set(normal_cases['image_path'])) + len(set(abnormal_cases['image_path']))))"
      ],
      "id": "plastic-master",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total de dados após a filtração: 13952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secure-alignment"
      },
      "source": [
        "# removendo as imagens repetidas\n",
        "normal_data = normal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
        "abnormal_data = abnormal_cases[['image_path', 'class_name']].drop_duplicates(subset = 'image_path', )\n",
        "\n",
        "# criando dataframes especifos com caminhos para as imagens e rótulos\n",
        "normal_data['target'] = 'normal'\n",
        "abnormal_data['target'] = 'abnormal'"
      ],
      "id": "secure-alignment",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj0RjdWeCzpm",
        "outputId": "1b4b5178-08a7-4408-de96-308db24c40d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# visualizando a quantidade de exemplos após a remoção de duplicatas\n",
        "abnormal_data['class_name'].value_counts()"
      ],
      "id": "oj0RjdWeCzpm",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pleural thickening    901\n",
              "Pulmonary fibrosis    742\n",
              "Lung Opacity          427\n",
              "Nodule/Mass           339\n",
              "Pleural effusion      328\n",
              "Calcification         167\n",
              "Infiltration          163\n",
              "ILD                   152\n",
              "Consolidation          59\n",
              "Atelectasis            41\n",
              "Pneumothorax           27\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sudden-platform",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea0288e-2ffa-4861-b28a-5dd7f3842793"
      },
      "source": [
        "print('quantidade de dados rotulados como normais:', len(normal_data))\n",
        "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
      ],
      "id": "sudden-platform",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de dados rotulados como normais: 10606\n",
            "quantidade de dados rotulados como anormais: 3346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prompt-embassy"
      },
      "source": [
        "# removendo 69% dos casos normais para balancear os dados\n",
        "normal, _ = train_test_split(normal_data, test_size = 0.90, random_state = 42)"
      ],
      "id": "prompt-embassy",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sitting-works",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648acf04-7c28-40fc-93ab-ab0f635c56f9"
      },
      "source": [
        "print('quantidade de dados rotulados como normais:', len(normal))\n",
        "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
      ],
      "id": "sitting-works",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de dados rotulados como normais: 1060\n",
            "quantidade de dados rotulados como anormais: 3346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "funny-document"
      },
      "source": [
        "# concatenando os dataframes de casos normais e anormais\n",
        "full_data = pd.concat([normal, abnormal_data])"
      ],
      "id": "funny-document",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "golden-reading"
      },
      "source": [
        "# misturando todos os dados do dataframe e reiniciando os valores dos índices \n",
        "full_data = full_data.sample(frac = 1, axis = 0, random_state = 42).reset_index(drop=True)"
      ],
      "id": "golden-reading",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medium-brand"
      },
      "source": [
        "# separando os dados de treinamento e de teste\n",
        "train_df, test_df = train_test_split(full_data, stratify = full_data['target'],\n",
        "                                     test_size = 0.2, random_state = 42)"
      ],
      "id": "medium-brand",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extraordinary-retrieval"
      },
      "source": [
        "# separando os dados de validação dos dados de treinamento\n",
        "train_df, validation_df = train_test_split(train_df, stratify = train_df['target'],\n",
        "                                           test_size = 0.2, random_state = 42)"
      ],
      "id": "extraordinary-retrieval",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dependent-collins",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f876955-cfd0-4b66-cb7c-5a6eff0da540"
      },
      "source": [
        "# visualizando a quantidade de dados\n",
        "print('quantidade de imagens de treinamento:', len(train_df['image_path']))\n",
        "print('quantidade de rótulos de treinamento:', len(train_df['class_name']))\n",
        "print('quantidade de imagens de teste:', len(test_df['image_path']))\n",
        "print('quantidade de rótulos de teste:', len(test_df['class_name']))\n",
        "print('quantidade de imagens de validação:', len(validation_df['image_path']))\n",
        "print('quantidade de rótulos de validação:', len(validation_df['class_name']))"
      ],
      "id": "dependent-collins",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quantidade de imagens de treinamento: 2819\n",
            "quantidade de rótulos de treinamento: 2819\n",
            "quantidade de imagens de teste: 882\n",
            "quantidade de rótulos de teste: 882\n",
            "quantidade de imagens de validação: 705\n",
            "quantidade de rótulos de validação: 705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "western-current"
      },
      "source": [
        "### Aplicando mudança de escala típica"
      ],
      "id": "western-current"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "requested-pakistan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3fdd40f-a137-4641-ce58-67f69f4fb399"
      },
      "source": [
        "# normalizando as imagens de treinamento e aplicando aumento de dados\n",
        "image_generator = ImageDataGenerator(rescale = 1./255.,\n",
        "                                     rotation_range = 10, zoom_range = 0.2)\n",
        "\n",
        "# criando o gerador de imagens de treinamento \n",
        "train_generator = image_generator.flow_from_dataframe(\n",
        "                                                      dataframe = train_df,\n",
        "                                                      directory = '',\n",
        "                                                      x_col = 'image_path',\n",
        "                                                      y_col = 'class_name',\n",
        "                                                      batch_size = 32,\n",
        "                                                      seed = 42,\n",
        "                                                      shuffle = True,\n",
        "                                                      class_mode = 'categorical',\n",
        "                                                      target_size = (256, 256))\n",
        "\n",
        "# normalizando as imagens de teste e validação\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255.)\n",
        "\n",
        "# criando o gerador de imagens de validação \n",
        "valid_generator = test_datagen.flow_from_dataframe(\n",
        "                                                      dataframe = validation_df,\n",
        "                                                      directory = '.', \n",
        "                                                      x_col = 'image_path',\n",
        "                                                      y_col = 'class_name',\n",
        "                                                      batch_size = 32,\n",
        "                                                      seed = 42,\n",
        "                                                      shuffle = True,\n",
        "                                                      class_mode = 'categorical',\n",
        "                                                      target_size = (256, 256))\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "                                                  dataframe = test_df, \n",
        "                                                  directory = '.',\n",
        "                                                  x_col = 'image_path',\n",
        "                                                  y_col = 'class_name',\n",
        "                                                  batch_size = 32,\n",
        "                                                  seed = 42,\n",
        "                                                  shuffle = True,\n",
        "                                                  class_mode = 'categorical',\n",
        "                                                  target_size = (256, 256))"
      ],
      "id": "requested-pakistan",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2819 validated image filenames belonging to 12 classes.\n",
            "Found 705 validated image filenames belonging to 12 classes.\n",
            "Found 882 validated image filenames belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3sWaxpVkJCZ",
        "outputId": "2381f909-03e7-4084-c0d1-9f75032a45a9"
      },
      "source": [
        "# visualizando a ordem númerica das classes\n",
        "train_generator.class_indices"
      ],
      "id": "g3sWaxpVkJCZ",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Atelectasis': 0,\n",
              " 'Calcification': 1,\n",
              " 'Consolidation': 2,\n",
              " 'ILD': 3,\n",
              " 'Infiltration': 4,\n",
              " 'Lung Opacity': 5,\n",
              " 'No finding': 6,\n",
              " 'Nodule/Mass': 7,\n",
              " 'Pleural effusion': 8,\n",
              " 'Pleural thickening': 9,\n",
              " 'Pneumothorax': 10,\n",
              " 'Pulmonary fibrosis': 11}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compatible-fetish"
      },
      "source": [
        "### Preparando a rede neural convolucional"
      ],
      "id": "compatible-fetish"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anonymous-rotation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532ea00c-d86c-461b-bb45-42568629b163"
      },
      "source": [
        "# baixando os pesos treinados da rede inception\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ],
      "id": "anonymous-rotation",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-16 00:29:43--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.203.128, 74.125.20.128, 74.125.197.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.203.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/tmp/inception_v3_w 100%[===================>]  83.84M  73.9MB/s    in 1.1s    \n",
            "\n",
            "2021-04-16 00:29:45 (73.9 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compact-knife"
      },
      "source": [
        "# referenciando o local em que os pesos estão armazenados\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "# carregando a arquitetura inception pré-treinada\n",
        "pre_trained_model = InceptionV3(input_shape = (256, 256, 3), \n",
        "                                include_top = False, \n",
        "                                weights = None)\n",
        "\n",
        "# carregando os pesos treinados com outros dados \n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "# obtendo a última camada como sendo a nomeada por 'mixed7'\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "last_output = last_layer.output"
      ],
      "id": "compact-knife",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "double-sound"
      },
      "source": [
        "x = layers.GlobalAveragePooling2D()(last_output)\n",
        "# adicionando uma camada densa com 512 neurônios\n",
        "x = layers.Dense(units = 512, activation = tf.nn.relu)(x)     \n",
        "# conecatando a rede uma camada com 128 neurônios e função de ativação relu\n",
        "x = layers.Dense(units = 256, activation = tf.nn.relu)(x) \n",
        "# aplicando uma camada de dropout com uma taxa de 20% (normalização)\n",
        "x = layers.Dropout(rate = 0.2)(x)      \n",
        "# adicionando uma camada de saída com um neurônio e uma função de ativação sigmoide\n",
        "x = layers.Dense  (units = 12, activation = tf.nn.softmax)(x)    \n",
        "\n",
        "# conecatando as camadas definidas acima com a arquitetura inception\n",
        "model = Model(pre_trained_model.input, x) \n",
        "\n",
        "# compilando a rede \n",
        "model.compile(optimizer = optimizers.RMSprop(learning_rate = 0.0001), loss = 'categorical_crossentropy', \n",
        "              metrics = ['acc'])"
      ],
      "id": "double-sound",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_22aw_XQlYG-"
      },
      "source": [
        "# definindo as flags iniciais  \n",
        "pre_trained_model.trainable = True\n",
        "set_trainable = False\n",
        "\n",
        "# para a arquitetura inception, a rede será retreinada a partir da camada 'mixed6'\n",
        "for layer in pre_trained_model.layers:\n",
        "    if layer.name == 'mixed6':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ],
      "id": "_22aw_XQlYG-",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "found-baker"
      },
      "source": [
        "# definindo o caminho pelo qual os pesos serão armazenados \n",
        "filepath = \"transferlearning_weights.hdf5\"\n",
        "# callback para salvar o melhor valor dos pesos em relação ao desempenho com os dados de validação \n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max')"
      ],
      "id": "found-baker",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forward-guide"
      },
      "source": [
        "# definindo um array de callbacks\n",
        "callbacks = [checkpoint]"
      ],
      "id": "forward-guide",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vital-constitutional",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80cac08f-6423-4c07-efab-2f82cc138475"
      },
      "source": [
        "# treinando a rede neural convolucional\n",
        "history = model.fit_generator(train_generator, steps_per_epoch = 2819 // 32, \n",
        "                              validation_data = valid_generator, validation_steps = 705 // 32,\n",
        "                              callbacks = callbacks, epochs = 50, use_multiprocessing = True,\n",
        "                              workers = 8)"
      ],
      "id": "vital-constitutional",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 2.1799 - acc: 0.2597WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 335s 4s/step - loss: 2.1778 - acc: 0.2604 - val_loss: 1.9109 - val_acc: 0.3793\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.37926, saving model to transferlearning_weights.hdf5\n",
            "Epoch 2/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7571 - acc: 0.3952WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 88s 930ms/step - loss: 1.7567 - acc: 0.3953 - val_loss: 1.8912 - val_acc: 0.4020\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.37926 to 0.40199, saving model to transferlearning_weights.hdf5\n",
            "Epoch 3/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.6582 - acc: 0.4205WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 89s 940ms/step - loss: 1.6579 - acc: 0.4206 - val_loss: 2.0443 - val_acc: 0.3764\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.40199\n",
            "Epoch 4/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.6106 - acc: 0.4306WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 89s 937ms/step - loss: 1.6101 - acc: 0.4309 - val_loss: 1.8905 - val_acc: 0.4304\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.40199 to 0.43040, saving model to transferlearning_weights.hdf5\n",
            "Epoch 5/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.4524 - acc: 0.4979WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 89s 941ms/step - loss: 1.4526 - acc: 0.4979 - val_loss: 1.7241 - val_acc: 0.4418\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.43040 to 0.44176, saving model to transferlearning_weights.hdf5\n",
            "Epoch 6/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.3713 - acc: 0.5325WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 89s 941ms/step - loss: 1.3718 - acc: 0.5322 - val_loss: 1.9178 - val_acc: 0.3977\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.44176\n",
            "Epoch 7/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.3300 - acc: 0.5252WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 945ms/step - loss: 1.3298 - acc: 0.5252 - val_loss: 1.8514 - val_acc: 0.4077\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.44176\n",
            "Epoch 8/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.2210 - acc: 0.5673WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 944ms/step - loss: 1.2209 - acc: 0.5674 - val_loss: 1.9252 - val_acc: 0.4205\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.44176\n",
            "Epoch 9/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.0761 - acc: 0.6298WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 91s 964ms/step - loss: 1.0764 - acc: 0.6298 - val_loss: 2.0392 - val_acc: 0.4290\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.44176\n",
            "Epoch 10/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.0054 - acc: 0.6488WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 956ms/step - loss: 1.0054 - acc: 0.6488 - val_loss: 2.6012 - val_acc: 0.3835\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.44176\n",
            "Epoch 11/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.8911 - acc: 0.6932WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 957ms/step - loss: 0.8912 - acc: 0.6931 - val_loss: 3.1898 - val_acc: 0.4318\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.44176\n",
            "Epoch 12/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.7397 - acc: 0.7686WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 950ms/step - loss: 0.7403 - acc: 0.7683 - val_loss: 2.8232 - val_acc: 0.4545\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.44176 to 0.45455, saving model to transferlearning_weights.hdf5\n",
            "Epoch 13/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.7014 - acc: 0.7581WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 957ms/step - loss: 0.7016 - acc: 0.7581 - val_loss: 2.7092 - val_acc: 0.4261\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.45455\n",
            "Epoch 14/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.6262 - acc: 0.7999WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 952ms/step - loss: 0.6265 - acc: 0.7998 - val_loss: 2.6385 - val_acc: 0.3963\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.45455\n",
            "Epoch 15/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.5674 - acc: 0.8157WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 976ms/step - loss: 0.5672 - acc: 0.8157 - val_loss: 2.8610 - val_acc: 0.3494\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.45455\n",
            "Epoch 16/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.4756 - acc: 0.8419WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 947ms/step - loss: 0.4760 - acc: 0.8418 - val_loss: 2.8693 - val_acc: 0.3849\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.45455\n",
            "Epoch 17/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.4641 - acc: 0.8538WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 956ms/step - loss: 0.4640 - acc: 0.8537 - val_loss: 3.1976 - val_acc: 0.4077\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.45455\n",
            "Epoch 18/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.4462 - acc: 0.8538WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 91s 960ms/step - loss: 0.4461 - acc: 0.8538 - val_loss: 3.1145 - val_acc: 0.3821\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.45455\n",
            "Epoch 19/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.3241 - acc: 0.9044WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 92s 975ms/step - loss: 0.3245 - acc: 0.9041 - val_loss: 3.0143 - val_acc: 0.4247\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.45455\n",
            "Epoch 20/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "87/88 [============================>.] - ETA: 0s - loss: 0.3009 - acc: 0.9098WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 951ms/step - loss: 0.3020 - acc: 0.9093 - val_loss: 3.1732 - val_acc: 0.3977\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.45455\n",
            "Epoch 21/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.3160 - acc: 0.8996WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 948ms/step - loss: 0.3160 - acc: 0.8996 - val_loss: 3.0233 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.45455\n",
            "Epoch 22/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.2637 - acc: 0.9176WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 954ms/step - loss: 0.2638 - acc: 0.9176 - val_loss: 3.6043 - val_acc: 0.4190\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.45455\n",
            "Epoch 23/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.2458 - acc: 0.9151WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 958ms/step - loss: 0.2459 - acc: 0.9151 - val_loss: 3.5045 - val_acc: 0.4148\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.45455\n",
            "Epoch 24/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.2040 - acc: 0.9334WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 949ms/step - loss: 0.2043 - acc: 0.9333 - val_loss: 3.5933 - val_acc: 0.4560\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.45455 to 0.45597, saving model to transferlearning_weights.hdf5\n",
            "Epoch 25/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1992 - acc: 0.9400WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 948ms/step - loss: 0.1997 - acc: 0.9398 - val_loss: 3.1373 - val_acc: 0.3778\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.45597\n",
            "Epoch 26/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1897 - acc: 0.9409WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 947ms/step - loss: 0.1898 - acc: 0.9408 - val_loss: 3.5045 - val_acc: 0.4290\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.45597\n",
            "Epoch 27/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1781 - acc: 0.9559WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 955ms/step - loss: 0.1782 - acc: 0.9558 - val_loss: 3.9296 - val_acc: 0.4531\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.45597\n",
            "Epoch 28/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1941 - acc: 0.9429WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 948ms/step - loss: 0.1942 - acc: 0.9428 - val_loss: 4.0874 - val_acc: 0.3523\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.45597\n",
            "Epoch 29/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1660 - acc: 0.9419WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 91s 962ms/step - loss: 0.1663 - acc: 0.9418 - val_loss: 3.3724 - val_acc: 0.3849\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.45597\n",
            "Epoch 30/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1485 - acc: 0.9520WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 952ms/step - loss: 0.1486 - acc: 0.9520 - val_loss: 3.5822 - val_acc: 0.3707\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.45597\n",
            "Epoch 31/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1823 - acc: 0.9483WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 947ms/step - loss: 0.1822 - acc: 0.9483 - val_loss: 4.2858 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.45597\n",
            "Epoch 32/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1555 - acc: 0.9529WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 948ms/step - loss: 0.1555 - acc: 0.9529 - val_loss: 3.8068 - val_acc: 0.4020\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.45597\n",
            "Epoch 33/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1338 - acc: 0.9624WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 946ms/step - loss: 0.1339 - acc: 0.9623 - val_loss: 4.4055 - val_acc: 0.3693\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.45597\n",
            "Epoch 34/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1203 - acc: 0.9582WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 960ms/step - loss: 0.1205 - acc: 0.9581 - val_loss: 4.1848 - val_acc: 0.3991\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.45597\n",
            "Epoch 35/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1412 - acc: 0.9505WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 949ms/step - loss: 0.1412 - acc: 0.9505 - val_loss: 3.7762 - val_acc: 0.3565\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.45597\n",
            "Epoch 36/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1214 - acc: 0.9582WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 952ms/step - loss: 0.1214 - acc: 0.9582 - val_loss: 4.2868 - val_acc: 0.3636\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.45597\n",
            "Epoch 37/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1501 - acc: 0.9453WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 954ms/step - loss: 0.1499 - acc: 0.9454 - val_loss: 4.2055 - val_acc: 0.4148\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.45597\n",
            "Epoch 38/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1114 - acc: 0.9614WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 952ms/step - loss: 0.1114 - acc: 0.9615 - val_loss: 3.6590 - val_acc: 0.3537\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.45597\n",
            "Epoch 39/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1117 - acc: 0.9624WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 91s 967ms/step - loss: 0.1117 - acc: 0.9624 - val_loss: 3.6933 - val_acc: 0.3920\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.45597\n",
            "Epoch 40/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1106 - acc: 0.9667WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 960ms/step - loss: 0.1109 - acc: 0.9667 - val_loss: 3.9292 - val_acc: 0.3878\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.45597\n",
            "Epoch 41/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0942 - acc: 0.9755WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 91s 956ms/step - loss: 0.0943 - acc: 0.9754 - val_loss: 4.8449 - val_acc: 0.4347\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.45597\n",
            "Epoch 42/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0880 - acc: 0.9704WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 964ms/step - loss: 0.0881 - acc: 0.9704 - val_loss: 3.8212 - val_acc: 0.3622\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.45597\n",
            "Epoch 43/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0858 - acc: 0.9713WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 949ms/step - loss: 0.0860 - acc: 0.9713 - val_loss: 4.7354 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.45597\n",
            "Epoch 44/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0843 - acc: 0.9737WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 952ms/step - loss: 0.0846 - acc: 0.9737 - val_loss: 4.3216 - val_acc: 0.3849\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.45597\n",
            "Epoch 45/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0845 - acc: 0.9731WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 956ms/step - loss: 0.0846 - acc: 0.9730 - val_loss: 4.4472 - val_acc: 0.4048\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.45597\n",
            "Epoch 46/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0826 - acc: 0.9736WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 947ms/step - loss: 0.0828 - acc: 0.9735 - val_loss: 4.1029 - val_acc: 0.3182\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.45597\n",
            "Epoch 47/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0741 - acc: 0.9807WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 953ms/step - loss: 0.0743 - acc: 0.9806 - val_loss: 5.3660 - val_acc: 0.4006\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.45597\n",
            "Epoch 48/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0917 - acc: 0.9632WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 950ms/step - loss: 0.0916 - acc: 0.9632 - val_loss: 5.1688 - val_acc: 0.4290\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.45597\n",
            "Epoch 49/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1041 - acc: 0.9671WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 951ms/step - loss: 0.1043 - acc: 0.9670 - val_loss: 4.4692 - val_acc: 0.4347\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.45597\n",
            "Epoch 50/50\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0962 - acc: 0.9715WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "88/88 [==============================] - 90s 952ms/step - loss: 0.0961 - acc: 0.9715 - val_loss: 4.0239 - val_acc: 0.3608\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.45597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_XSvn8zzsQh"
      },
      "source": [
        "### Salvando o modelo"
      ],
      "id": "g_XSvn8zzsQh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8BnX8Xj5hpN"
      },
      "source": [
        "# carregando o melhor peso obtido para o modelo\n",
        "best_model = model\n",
        "best_model.load_weights('/content/transferlearning_weights.hdf5')"
      ],
      "id": "t8BnX8Xj5hpN",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKCqLdTn-WsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7aaf852-0667-4afb-ff1d-e4d5ff1e0ee6"
      },
      "source": [
        "# salvando os dois modelos obtidos durante o treinamento\n",
        "model.save('model1')\n",
        "best_model.save('model2')"
      ],
      "id": "yKCqLdTn-WsT",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model1/assets\n",
            "INFO:tensorflow:Assets written to: model2/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h42jjoyHY-61",
        "outputId": "1a28c3d7-6b8f-4caa-d0f5-29d842b09e2f"
      },
      "source": [
        "# testando a capacidade de generalização do modelo\n",
        "model.evaluate(test_generator)"
      ],
      "id": "h42jjoyHY-61",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28/28 [==============================] - 793s 29s/step - loss: 3.7552 - acc: 0.4127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.755213975906372, 0.4126984179019928]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKNuDpmj0FnD"
      },
      "source": [
        "### Métricas de avaliação do modelo"
      ],
      "id": "mKNuDpmj0FnD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hi3nMWd3m6H",
        "outputId": "c7827cfe-610d-49e1-92e1-4138444033f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "# carregando os dados de teste\n",
        "for i in range(0, 42):\n",
        "  (x1, y1) = test_generator[i]\n",
        "  if i == 0:\n",
        "    x, y = x1, y1\n",
        "  else:\n",
        "    x = np.concatenate((x, x1))\n",
        "    y = np.concatenate((y, y1))"
      ],
      "id": "5Hi3nMWd3m6H",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-46b647938d50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# carregando os dados de teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m                              \u001b[0;34m'but the Sequence '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                              'has length {length}'.format(idx=idx,\n\u001b[0;32m---> 57\u001b[0;31m                                                           length=len(self)))\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_batches_seen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 28, but the Sequence has length 28"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrWys7Kh36m2"
      },
      "source": [
        "# realizando a predição para os dados de teste\n",
        "predict = model.predict(x)"
      ],
      "id": "xrWys7Kh36m2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqjrfvXFaNgH"
      },
      "source": [
        "predict[1]"
      ],
      "id": "DqjrfvXFaNgH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w56rK5yzaidg"
      },
      "source": [
        ""
      ],
      "id": "w56rK5yzaidg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojGyKsW_by6U"
      },
      "source": [
        "for i in predict:\n",
        "  print(i)\n",
        "  break"
      ],
      "id": "ojGyKsW_by6U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKrkv_WoajpJ"
      },
      "source": [
        "a = thresholds(0.5,predict)\n",
        "a[0]"
      ],
      "id": "SKrkv_WoajpJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYGlLFgGddvp"
      },
      "source": [
        "a = np.array(a)\n",
        "a = a.reshape((1327, 12))\n",
        "a.shape"
      ],
      "id": "gYGlLFgGddvp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FWqcM9Lgh9l"
      },
      "source": [
        "y[0]"
      ],
      "id": "2FWqcM9Lgh9l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xvDNlj637Q8"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "def thresholds(limiar, predict):\n",
        "  '''predição para diferentes thresholds'''\n",
        "\n",
        "  aux = list()\n",
        "  for value in predict:\n",
        "    for i in range(0, len(value)):\n",
        "      if value[i] > limiar:\n",
        "        aux.append(1)\n",
        "      else:\n",
        "        aux.append(0)\n",
        "  \n",
        "  aux = np.array(aux)\n",
        "  aux = aux.reshape((len(predict), len(predict[0]))).astype('float32')\n",
        "  \n",
        "  return aux\n",
        "\n",
        "def precision_recall_accuracy_curve(predict, y):\n",
        "  ''' Relaciona a curva da Precisão, Sensibilidade e Acurácia em relação a diferentes Thresholds'''\n",
        "\n",
        "  limiares = np.arange(0, 1, 0.05)\n",
        "  predicts = []\n",
        "  precisions = []\n",
        "  recalls = []\n",
        "  accuracy = []\n",
        "  flag = 0\n",
        "  for i in limiares:\n",
        "    predicts.append(thresholds(i, predict))\n",
        "    precisions.append(precision_score(predicts[flag], y))\n",
        "    recalls.append(recall_score(predicts[flag], y))\n",
        "    accuracy.append(accuracy_score(predicts[flag], y))\n",
        "    flag += 1\n",
        "  \n",
        "  return precisions, recalls, accuracy\n",
        "\n",
        "def plot_precision_recall_accuracy_curve(precisions, recalls, accuracy):\n",
        "  '''Plotando a curva de Precisão, Sensibilidade e Acurácia'''\n",
        "\n",
        "  plt.figure(figsize = (10,5))\n",
        "  plt.plot(np.arange(0, 1, 0.05), precisions, label = 'Precision')\n",
        "  plt.plot(np.arange(0, 1, 0.05), recalls, label = 'Recall')\n",
        "  plt.plot(np.arange(0, 1, 0.05), accuracy, label = 'Accuracy')\n",
        "  plt.title('Precisão, Sensibilidade e Acurácia para diferentes Thresholds')\n",
        "  plt.xlabel('Thresholds')\n",
        "  plt.legend()\n",
        "  plt.savefig('curve-analysis')\n",
        "\n",
        "  return None\n",
        "\n",
        "def best_metrics(threshold, predict, y):\n",
        "  '''Melhores valores para o threshold escolhido'''\n",
        "\n",
        "  predict_ = thresholds(threshold, predict)\n",
        "  print('Matriz de Confusão:\\n', confusion_matrix(np.ndarray.tolist(predict_.astype('int')),\n",
        "                                                  np.ndarray.tolist(y.astype('int')), labels = ))\n",
        "  print('Acurácia:', accuracy_score(predict_, y))\n",
        "  print('Precisão', precision_score(predict_, y))\n",
        "  print('Sensibilidade:', recall_score(predict_, y)) \n",
        "  print('F1_Score:', f1_score(predict_, y))\n",
        "\n",
        "  return None"
      ],
      "id": "5xvDNlj637Q8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Snx8znibvf"
      },
      "source": [
        "train_generator.class_indices\n",
        "labels = ['Atelectasis', 'Calcification', 'Consolidation', 'ILD', 'Infiltration', 'Lung Opacity',\n",
        "          ]"
      ],
      "id": "g4Snx8znibvf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWGYmc9h380K"
      },
      "source": [
        "# plotando a curva da Precisão, Sensibilidade e Acurácia \n",
        "precisions, recalls, accuracy = precision_recall_accuracy_curve(predict[:,0], y[:,0])\n",
        "plot_precision_recall_accuracy_curve(precisions, recalls, accuracy)"
      ],
      "id": "dWGYmc9h380K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQhR8TcO2GuE"
      },
      "source": [
        "# plotando a curva da Precisão, Sensibilidade e Acurácia \n",
        "precisions, recalls, accuracy = precision_recall_accuracy_curve(predict[:,1], y[:,1])\n",
        "plot_precision_recall_accuracy_curve(precisions, recalls, accuracy)"
      ],
      "id": "aQhR8TcO2GuE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0RmqbxK4Ntl"
      },
      "source": [
        "# analisando as melhores métricas encontradas para o modelo\n",
        "best_metrics(threshold = 0.60, predict = predict, y = y)"
      ],
      "id": "s0RmqbxK4Ntl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ti523y59T1c"
      },
      "source": [
        "# analisando as melhores métricas encontradas para o modelo\n",
        "best_metrics(threshold = 0.60, predict = predict[:,1], y = y[:,1])"
      ],
      "id": "6Ti523y59T1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpxO4IFh-bsn"
      },
      "source": [
        "# visualizando o ganho de acurácia durante o treinamento\n",
        "plt.figure(figsize = (15,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig('model-accuracy')\n",
        "\n",
        "# visualizando o decaimento da função de custo durante o treinamento \n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig('model-loss')"
      ],
      "id": "MpxO4IFh-bsn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtdpuJSI_hf3"
      },
      "source": [
        "### Algoritmo Grad Cam"
      ],
      "id": "KtdpuJSI_hf3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geZvnHrX_kSo"
      },
      "source": [
        "import urllib.request as url\n",
        "\n",
        "link = 'https://raw.githubusercontent.com/Alyssonmach/class-activation-maps/main/assets/grad_cam.py'\n",
        "file_ = 'grad_cam.py'\n",
        "url.urlretrieve(link, file_)\n",
        "\n",
        "from grad_cam import get_img_array, make_gradcam_heatmap, save_and_display_gradcam"
      ],
      "id": "geZvnHrX_kSo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5DdAGhR_nTJ"
      },
      "source": [
        "# tamanho padrão das imagens do modelo\n",
        "img_size = (256, 256)\n",
        "# importando os parâmetros de pré-processamento da rede\n",
        "preprocess_input = tf.keras.applications.inception_v3.preprocess_input\n",
        "# definindo a última camada da rede a ser considerada\n",
        "last_conv_layer = 'mixed7'\n",
        "model_builder = model\n",
        "# removendo a função de ativação da última camada\n",
        "model_builder.layers[-1].activation = None"
      ],
      "id": "O5DdAGhR_nTJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLmqcoaKB2HA"
      },
      "source": [
        "img, lbl = x[0], y[0]\n",
        "\n",
        "img_path = tf.keras.preprocessing.image.array_to_img(img)\n",
        "\n",
        "# preparando a imagem\n",
        "img_array = preprocess_input(get_img_array(img_path, img_size))\n",
        "\n",
        "# obtendo a predição do modelo\n",
        "preds = model_builder.predict(img_array)\n",
        "print('Classe prevista: {}'.format(lbl))\n",
        "\n",
        "# gerando o mapa de ativação de classe (Grad-Cam)\n",
        "heatmap = make_gradcam_heatmap(img_array, model_builder, last_conv_layer,\n",
        "                               pred_index = 0)\n",
        "\n",
        "# resultado final do algoritmo Grad-Cam\n",
        "heatmap = save_and_display_gradcam(img_path, heatmap, cam_path = 'image1.png')"
      ],
      "id": "zLmqcoaKB2HA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h7uN3Zg_q-k"
      },
      "source": [
        "img, lbl = x[5], y[5]\n",
        "\n",
        "img_path = tf.keras.preprocessing.image.array_to_img(img)\n",
        "\n",
        "# preparando a imagem\n",
        "img_array = preprocess_input(get_img_array(img_path, img_size))\n",
        "\n",
        "# obtendo a predição do modelo\n",
        "preds = model_builder.predict(img_array)\n",
        "print('Classe prevista: {}'.format(lbl))\n",
        "\n",
        "# gerando o mapa de ativação de classe (Grad-Cam)\n",
        "heatmap = make_gradcam_heatmap(img_array, model_builder, last_conv_layer,\n",
        "                               pred_index = 0)\n",
        "\n",
        "# resultado final do algoritmo Grad-Cam\n",
        "heatmap = save_and_display_gradcam(img_path, heatmap, cam_path = 'image2.png')"
      ],
      "id": "6h7uN3Zg_q-k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNOt2sAZIRQ7"
      },
      "source": [
        "img, lbl = x[6], y[6]\n",
        "\n",
        "img_path = tf.keras.preprocessing.image.array_to_img(img)\n",
        "\n",
        "# preparando a imagem\n",
        "img_array = preprocess_input(get_img_array(img_path, img_size))\n",
        "\n",
        "# obtendo a predição do modelo\n",
        "preds = model_builder.predict(img_array)\n",
        "print('Classe prevista: {}'.format(lbl))\n",
        "\n",
        "# gerando o mapa de ativação de classe (Grad-Cam)\n",
        "heatmap = make_gradcam_heatmap(img_array, model_builder, last_conv_layer,\n",
        "                               pred_index = 0)\n",
        "\n",
        "# resultado final do algoritmo Grad-Cam\n",
        "heatmap = save_and_display_gradcam(img_path, heatmap, cam_path = 'image3.png')"
      ],
      "id": "BNOt2sAZIRQ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXSpcVY_IX1C"
      },
      "source": [
        "img, lbl = x[56], y[56]\n",
        "\n",
        "img_path = tf.keras.preprocessing.image.array_to_img(img)\n",
        "\n",
        "# preparando a imagem\n",
        "img_array = preprocess_input(get_img_array(img_path, img_size))\n",
        "\n",
        "# obtendo a predição do modelo\n",
        "preds = model_builder.predict(img_array)\n",
        "print('Classe prevista: {}'.format(lbl))\n",
        "\n",
        "# gerando o mapa de ativação de classe (Grad-Cam)\n",
        "heatmap = make_gradcam_heatmap(img_array, model_builder, last_conv_layer,\n",
        "                               pred_index = 0)\n",
        "\n",
        "# resultado final do algoritmo Grad-Cam\n",
        "heatmap = save_and_display_gradcam(img_path, heatmap, cam_path = 'image4.png')"
      ],
      "id": "xXSpcVY_IX1C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYd42MVY_e82"
      },
      "source": [
        "### Visualizando a arquitetura do modelo"
      ],
      "id": "kYd42MVY_e82"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LNMCgoR-y3d"
      },
      "source": [
        "# visualizando a arquitetura do modelo\n",
        "model.summary()"
      ],
      "id": "_LNMCgoR-y3d",
      "execution_count": null,
      "outputs": []
    }
  ]
}