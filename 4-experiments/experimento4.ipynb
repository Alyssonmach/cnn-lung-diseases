{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lasting-selection",
   "metadata": {},
   "source": [
    "# Experimento 4\n",
    "***\n",
    "- Analisando com todos os conjunto de dados\n",
    "- Analisando o efeito da estratégia de comitês "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-flash",
   "metadata": {},
   "source": [
    "### Importação dos pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-facility",
   "metadata": {},
   "source": [
    "### Pré-processamento no conjunto de dados 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coletando o caminho dos arquivos dos dados do hospital shenzen\n",
    "filelist_shenzen = glob.glob('/content/drive/MyDrive/ChinaSet_AllFiles/ChinaSet_AllFiles/CXR_png/*.png')\n",
    "# coletando o caminho dos arquivos dos dados do hospital montgomery\n",
    "filelist_montgomery = glob.glob('/content/drive/MyDrive/Montgomery/MontgomerySet/CXR_png/*.png')\n",
    "# juntando os dois datasets\n",
    "filelist = filelist_shenzen + filelist_montgomery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantidade de imagens disponíveis no dataset\n",
    "print('quantidade de imagens:', str(len(filelist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(file_list):\n",
    "    \n",
    "    # inicializando uma lista vazia\n",
    "    labels = []\n",
    "    \n",
    "    # iterando na lista de arquivos\n",
    "    for file in tqdm(file_list):\n",
    "        # detectando as classes presentes no nome da imagem\n",
    "        current_label = re.findall('[0-9]{4}_(.+?).png', file)\n",
    "        # adicionando a lista de rótulos as classes correspondentes a cada uma das imagens\n",
    "        labels.append(current_label[0])\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraindo os rótulos\n",
    "labels = extract_label(filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando a quantidade de rótulos\n",
    "print('quantidade de rótulos:', str(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um dataframe com os caminhos das imagens\n",
    "full_data = pd.DataFrame(filelist, columns = ['filepath'])\n",
    "# adicionando os rótulos em cada imagem\n",
    "full_data['target'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modificando o formato dos dados para float32\n",
    "dict_type = {'target': 'float32'}\n",
    "full_data = full_data.astype(dict_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando os dados de treinamento e de teste\n",
    "train_df, test_df = train_test_split(full_data, stratify = full_data['target'],\n",
    "                                     test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando os dados de validação dos dados de treinamento\n",
    "train_df, validation_df = train_test_split(train_df, stratify = train_df['target'],\n",
    "                                           test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando a quantidade de dados\n",
    "print('quantidade de imagens de treinamento:', len(train_df['filepath']))\n",
    "print('quantidade de rótulos de treinamento:', len(train_df['target']))\n",
    "print('quantidade de imagens de teste:', len(test_df['filepath']))\n",
    "print('quantidade de rótulos de teste:', len(test_df['target']))\n",
    "print('quantidade de imagens de validação:', len(validation_df['filepath']))\n",
    "print('quantidade de rótulos de validação:', len(validation_df['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-accreditation",
   "metadata": {},
   "source": [
    "### Aplicando mudança de escala típica no terceiro conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizando as imagens de treinamento e aplicando aumento de dados\n",
    "image_generator1 = ImageDataGenerator(rescale = 1./255., rotation_range = 10, zoom_range = 0.2)\n",
    "\n",
    "# criando o gerador de imagens de treinamento \n",
    "train_generator1 = image_generator1.flow_from_dataframe(\n",
    "                                                      dataframe = train_df,\n",
    "                                                      directory = '',\n",
    "                                                      x_col = 'filepath',\n",
    "                                                      y_col = 'target',\n",
    "                                                      batch_size = 32,\n",
    "                                                      seed = 42,\n",
    "                                                      shuffle = True,\n",
    "                                                      class_mode = 'raw',\n",
    "                                                      color_mode = 'rgb',\n",
    "                                                      target_size = (256, 256))\n",
    "# criando o gerador de imagens de validação \n",
    "valid_generator1 = image_generator1.flow_from_dataframe(\n",
    "                                                      dataframe = validation_df,\n",
    "                                                      directory = '.', \n",
    "                                                      x_col = 'filepath',\n",
    "                                                      y_col = 'target',\n",
    "                                                      batch_size = 32,\n",
    "                                                      seed = 42,\n",
    "                                                      shuffle = True,\n",
    "                                                      class_mode = 'raw',\n",
    "                                                      target_size = (256, 256))\n",
    "\n",
    "# normalizando as imagens de teste \n",
    "test_datagen1 = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "test_generator1 = test_datagen1.flow_from_dataframe(\n",
    "                                                  dataframe = test_df, \n",
    "                                                  directory = '.',\n",
    "                                                  x_col = 'filepath',\n",
    "                                                  y_col = 'target',\n",
    "                                                  batch_size = 32,\n",
    "                                                  seed = 42,\n",
    "                                                  shuffle = True,\n",
    "                                                  class_mode = 'raw',\n",
    "                                                  target_size = (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-weapon",
   "metadata": {},
   "source": [
    "### Pré-processamento no conjunto de dados 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-increase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo os dados de um arquivo csv\n",
    "dataframe = pd.read_csv('/content/drive/MyDrive/vinbigdata/train.csv')\n",
    "# criando uma coluna com os caminhos relativos as imagens\n",
    "dataframe['image_path'] = '/content/drive/MyDrive/vinbigdata/train/' + dataframe.image_id + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total de imagens disponíveis:', str(len(set(dataframe['image_path']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando os casos disponíveis\n",
    "dataframe['class_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo os casos não relativos a distúrbios pulmonares\n",
    "dataframe = dataframe[dataframe.class_name != 'Aortic enlargement']\n",
    "dataframe = dataframe[dataframe.class_name != 'Cardiomegaly']\n",
    "dataframe = dataframe[dataframe.class_name != 'Other lesion']\n",
    "dataframe = dataframe[dataframe.class_name != 'Consolidation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando os casos rotulados como normais e anormais\n",
    "normal_cases = dataframe[(dataframe.class_id == 14) & (dataframe.class_name == 'No finding')]\n",
    "abnormal_cases = dataframe[(dataframe.class_id != 14) & (dataframe.class_name != 'No finding')]\n",
    "\n",
    "print('total de dados após a filtração:', str(len(set(normal_cases['image_path'])) + len(set(abnormal_cases['image_path']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo as imagens repetidas\n",
    "normal_path = list(set(normal_cases['image_path']))\n",
    "abnormal_path = list(set(abnormal_cases['image_path']))\n",
    "\n",
    "# criando dataframes especifos com caminhos para as imagens e rótulos\n",
    "normal_data = pd.DataFrame(normal_path, columns = ['filepath'])\n",
    "normal_data['target'] = 0\n",
    "abnormal_data = pd.DataFrame(abnormal_path, columns = ['filepath'])\n",
    "abnormal_data['target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('quantidade de dados rotulados como normais:', len(normal_data))\n",
    "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removendo 69% dos casos normais para balancear os dados\n",
    "normal, _ = train_test_split(normal_data, test_size = 0.69, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('quantidade de dados rotulados como normais:', len(normal))\n",
    "print('quantidade de dados rotulados como anormais:', len(abnormal_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenando os dataframes de casos normais e anormais\n",
    "full_data = pd.concat([normal, abnormal_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# misturando todos os dados do dataframe e reiniciando os valores dos índices \n",
    "full_data = full_data.sample(frac = 1, axis = 0, random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modificando o formato dos dados para float32\n",
    "dict_type = {'target': 'float32'}\n",
    "full_data = full_data.astype(dict_type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando os dados de treinamento e de teste\n",
    "train_df, test_df = train_test_split(full_data, stratify = full_data['target'],\n",
    "                                     test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando os dados de validação dos dados de treinamento\n",
    "train_df, validation_df = train_test_split(train_df, stratify = train_df['target'],\n",
    "                                           test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando a quantidade de dados\n",
    "print('quantidade de imagens de treinamento:', len(train_df['filepath']))\n",
    "print('quantidade de rótulos de treinamento:', len(train_df['target']))\n",
    "print('quantidade de imagens de teste:', len(test_df['filepath']))\n",
    "print('quantidade de rótulos de teste:', len(test_df['target']))\n",
    "print('quantidade de imagens de validação:', len(validation_df['filepath']))\n",
    "print('quantidade de rótulos de validação:', len(validation_df['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-financing",
   "metadata": {},
   "source": [
    "### Aplicando mundaça de escala típica no conjunto de dados 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizando as imagens de treinamento e aplicando aumento de dados\n",
    "image_generator1 = ImageDataGenerator(rescale = 1./255.,\n",
    "                                     rotation_range = 10, zoom_range = 0.2)\n",
    "\n",
    "# criando o gerador de imagens de treinamento \n",
    "train_generator1 = image_generator1.flow_from_dataframe(\n",
    "                                                      dataframe = train_df,\n",
    "                                                      directory = '',\n",
    "                                                      x_col = 'filepath',\n",
    "                                                      y_col = 'target',\n",
    "                                                      batch_size = 32,\n",
    "                                                      seed = 42,\n",
    "                                                      shuffle = True,\n",
    "                                                      class_mode = 'raw',\n",
    "                                                      target_size = (256, 256))\n",
    "# criando o gerador de imagens de validação \n",
    "valid_generator1 = image_generator1.flow_from_dataframe(\n",
    "                                                      dataframe = validation_df,\n",
    "                                                      directory = '.', \n",
    "                                                      x_col = 'filepath',\n",
    "                                                      y_col = 'target',\n",
    "                                                      batch_size = 32,\n",
    "                                                      seed = 42,\n",
    "                                                      shuffle = True,\n",
    "                                                      class_mode = 'raw',\n",
    "                                                      target_size = (256, 256))\n",
    "\n",
    "# normalizando as imagens de teste \n",
    "test_datagen1 = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "test_generator1 = test_datagen1.flow_from_dataframe(\n",
    "                                                  dataframe = test_df, \n",
    "                                                  directory = '.',\n",
    "                                                  x_col = 'filepath',\n",
    "                                                  y_col = 'target',\n",
    "                                                  batch_size = 32,\n",
    "                                                  seed = 42,\n",
    "                                                  shuffle = True,\n",
    "                                                  class_mode = 'raw',\n",
    "                                                  target_size = (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-acquisition",
   "metadata": {},
   "source": [
    "### Carregando os modelos salvos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando os modelos feitos para realizar a estratégia de análise por comitês \n",
    "model1 = tf.keras.models.load_model('/content/drive/MyDrive/experimentos/experimento1-dataset1-parte1/model2')\n",
    "model2 = tf.keras.models.load_model('/content/drive/MyDrive/experimentos/experimento1-dataset2-parte1/model2')\n",
    "model3 = tf.keras.models.load_model('/content/drive/MyDrive/experimentos/experimento2-dataset4/model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando os dados de teste \n",
    "for i in range(0, 42):\n",
    "  (x1, y1) = test_generator2[i]\n",
    "  if i == 0:\n",
    "    x, y = x1, y1\n",
    "  else:\n",
    "    x = np.concatenate((x, x1))\n",
    "    y = np.concatenate((y, y1))\n",
    "    \n",
    "(x1, y1) = test_generator1[0]\n",
    "(x2, y2) = test_generator1[1]\n",
    "(x3, y3) = test_generator1[2]\n",
    "(x4, y4) = test_generator1[3]\n",
    "(x5, y5) = test_generator1[4]\n",
    "\n",
    "x = np.concatenate((x, x1, x2, x3, x4, x5))\n",
    "y = np.concatenate((y, y1, y2, y3, y4, y5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realizando a predição nos dados de teste\n",
    "predict1 = model1.predict(x)\n",
    "predict2 = model2.predict(x)\n",
    "predict3 = model3.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo o valor médio das predições realizadas\n",
    "predict = (predict1 + predict2 + predict3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "def thresholds(limiar, predict):\n",
    "  '''predição para diferentes thresholds'''\n",
    "\n",
    "  predict_ = []\n",
    "  for i in predict:\n",
    "    if i > limiar:\n",
    "      predict_.append(1)\n",
    "    else:\n",
    "      predict_.append(0)\n",
    "  \n",
    "  return predict_\n",
    "\n",
    "def precision_recall_accuracy_curve(predict, y):\n",
    "  ''' Relaciona a curva da Precisão, Sensibilidade e Acurácia em relação a diferentes Thresholds'''\n",
    "\n",
    "  limiares = np.arange(0, 1, 0.05)\n",
    "  predicts = []\n",
    "  precisions = []\n",
    "  recalls = []\n",
    "  accuracy = []\n",
    "  flag = 0\n",
    "  for i in limiares:\n",
    "    predicts.append(thresholds(i, predict))\n",
    "    precisions.append(precision_score(predicts[flag], y))\n",
    "    recalls.append(recall_score(predicts[flag], y))\n",
    "    accuracy.append(accuracy_score(predicts[flag], y))\n",
    "    flag += 1\n",
    "  \n",
    "  return precisions, recalls, accuracy\n",
    "\n",
    "def plot_precision_recall_accuracy_curve(precisions, recalls, accuracy):\n",
    "  '''Plotando a curva de Precisão, Sensibilidade e Acurácia'''\n",
    "\n",
    "  plt.figure(figsize = (10,5))\n",
    "  plt.plot(np.arange(0, 1, 0.05), precisions, label = 'Precision')\n",
    "  plt.plot(np.arange(0, 1, 0.05), recalls, label = 'Recall')\n",
    "  plt.plot(np.arange(0, 1, 0.05), accuracy, label = 'Accuracy')\n",
    "  plt.title('Precisão, Sensibilidade e Acurácia para diferentes Thresholds')\n",
    "  plt.xlabel('Thresholds')\n",
    "  plt.legend()\n",
    "  plt.savefig('curve-analysis')\n",
    "\n",
    "  return None\n",
    "\n",
    "def best_metrics(threshold, predict, y):\n",
    "  '''Melhores valores para o threshold escolhido'''\n",
    "\n",
    "  predict_ = thresholds(threshold, predict)\n",
    "  print('Matriz de Confusão:\\n', confusion_matrix(predict_, y))\n",
    "  print('Acurácia:', accuracy_score(predict_, y))\n",
    "  print('Precisão', precision_score(predict_, y))\n",
    "  print('Sensibilidade:', recall_score(predict_, y)) \n",
    "  print('F1_Score:', f1_score(predict_, y))\n",
    "\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotando a curva da Precisão, Sensibilidade e Acurácia \n",
    "precisions, recalls, accuracy = precision_recall_accuracy_curve(predict, y)\n",
    "plot_precision_recall_accuracy_curve(precisions, recalls, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analisando as melhores métricas encontradas para o modelo\n",
    "best_metrics(threshold = 0.80, predict = predict, y = y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
