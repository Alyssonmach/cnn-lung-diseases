{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "integrated-aircraft",
   "metadata": {},
   "source": [
    "# Experimento 1\n",
    "***\n",
    "- Conjunto de Dados: NIH\n",
    "- Testando a equalização de histograma em comparação com a mudança típica de escala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-galaxy",
   "metadata": {},
   "source": [
    "### Importando os pacotes necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/Alyssonmach/cnn-lung-diseases/main/assets/cxr8_dados.py', 'cxr8_dados.py')\n",
    "from cxr8_dados import data_download, organize_csv, download_images, train_validation_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-apollo",
   "metadata": {},
   "source": [
    "### Pré-processamento nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baixando dataframe\n",
    "data_download('https://raw.githubusercontent.com/Alyssonmach/cnn-lung-diseases/main/dataframe-info.csv', 'dataframe-info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "# especificando o diretório com as imagens \n",
    "IMAGE_DIR = \"/content/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtendo o dataframe organizando\n",
    "dataframe = organize_csv('/content/dataframe-info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando o dataframe\n",
    "print(dataframe.head())\n",
    "print('dataframe shape:', dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# particionando o dataset em dados de treino, validação e teste  \n",
    "train_df, validation_df, test_df = train_validation_test_split(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando detalhes dos dados de treinamento\n",
    "print(train_df.head())\n",
    "print('train_df shape:', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando detalhes dos dados de treinamento\n",
    "print(validation_df.head())\n",
    "print('validation_df shape:', validation_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando detalhes dos dados de teste \n",
    "print(test_df.head())\n",
    "print('validation_df shape:', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-standing",
   "metadata": {},
   "source": [
    "### Aplicando equalização de histograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizando as imagens de treinamento e aplicando aumento de dados\n",
    "image_generator = ImageDataGenerator(samplewise_center = True, samplewise_std_normalization = True,\n",
    "                                     rotation_range = 10, zoom_range = 0.2)\n",
    "\n",
    "# criando o gerador de imagens de treinamento \n",
    "train_generator = image_generator.flow_from_dataframe(\n",
    "                                                      dataframe = train_df,\n",
    "                                                      directory = '',\n",
    "                                                      x_col = 'filepath',\n",
    "                                                      y_col = 'target',\n",
    "                                                      batch_size = 32,\n",
    "                                                      seed = 42,\n",
    "                                                      shuffle = True,\n",
    "                                                      class_mode = 'raw',\n",
    "                                                      target_size = (256, 256))\n",
    "# criando o gerador de imagens de validação \n",
    "valid_generator = image_generator.flow_from_dataframe(\n",
    "                                                      dataframe = validation_df,\n",
    "                                                      directory = '.', \n",
    "                                                      x_col = 'filepath',\n",
    "                                                      y_col = 'target',\n",
    "                                                      batch_size = 32,\n",
    "                                                      seed = 42,\n",
    "                                                      shuffle = True,\n",
    "                                                      class_mode = 'raw',\n",
    "                                                      target_size = (256, 256))\n",
    "\n",
    "# normalizando as imagens de teste \n",
    "test_datagen = ImageDataGenerator(samplewise_center = True, samplewise_std_normalization = True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "                                                  dataframe = test_df, \n",
    "                                                  directory = '.',\n",
    "                                                  x_col = 'filepath',\n",
    "                                                  y_col = 'target',\n",
    "                                                  batch_size = 32,\n",
    "                                                  seed = 42,\n",
    "                                                  shuffle = True,\n",
    "                                                  class_mode = 'raw',\n",
    "                                                  target_size = (256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-waterproof",
   "metadata": {},
   "source": [
    "### Preparando a rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baixando os pesos treinados da rede inception\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
    "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenciando o local em que os pesos estão armazenados\n",
    "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "# carregando a arquitetura inception pré-treinada\n",
    "pre_trained_model = InceptionV3(input_shape = (256, 256, 3), \n",
    "                                include_top = False, \n",
    "                                weights = None)\n",
    "\n",
    "# carregando os pesos treinados com outros dados \n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "# definindo as flags iniciais  \n",
    "pre_trained_model.trainable = True\n",
    "set_trainable = False\n",
    "\n",
    "# para a arquitetura inception, a rede será retreinada a partir da camada 'mixed6'\n",
    "for layer in pre_trained_model.layers:\n",
    "    if layer.name == 'mixed6':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "# visualizando a arquitetura definida\n",
    "pre_trained_model.summary()\n",
    "\n",
    "# obtendo a última camada como sendo a nomeada por 'mixed7'\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo uma camada de achatamento\n",
    "x = layers.Flatten()(last_output)\n",
    "# conecatando a rede uma camada com 1024 neurônios e função de ativação relu\n",
    "x = layers.Dense(units = 1024, activation = 'relu')(x)     \n",
    "# conecatando a rede uma camada com 128 neurônios e função de ativação relu\n",
    "x = layers.Dense(units = 512, activation = 'relu')(x) \n",
    "# aplicando uma camada de dropout com uma taxa de 20% (normalização)\n",
    "x = layers.Dropout(rate = 0.2)(x)                  \n",
    "# adicionando uma camada de saída com um neurônio e uma função de ativação sigmoide\n",
    "x = layers.Dense  (units = 1, activation = 'sigmoid')(x)           \n",
    "\n",
    "# conecatando as camadas definidas acima com a arquitetura inception\n",
    "model = Model(pre_trained_model.input, x) \n",
    "\n",
    "# compilando a rede \n",
    "model.compile(optimizer = optimizers.RMSprop(learning_rate = 0.0001), loss = 'binary_crossentropy', \n",
    "              metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo o caminho pelo qual os pesos serão armazenados \n",
    "filepath = \"transferlearning_weights.hdf5\"\n",
    "# callback para salvar o melhor valor dos pesos em relação ao desempenho com os dados de validação \n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo um array de callbacks\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinando a rede neural convolucional\n",
    "history = model.fit_generator(train_generator, steps_per_epoch = 4242 // 32, \n",
    "                              validation_data = valid_generator, validation_steps = 1061 // 32,\n",
    "                              callbacks = callbacks, epochs = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
